<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Abdul Hagi" />
  <title>Deep Reinforcement Learning for Developers</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="css/html.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Deep Reinforcement Learning for Developers</h1>
<p class="author">Abdul Hagi</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#chapter-1-introduction-to-reinforcement-learning"
id="toc-chapter-1-introduction-to-reinforcement-learning"><span
class="toc-section-number">1</span> Chapter 1: Introduction to
Reinforcement Learning</a>
<ul>
<li><a href="#part-1-understanding-the-fundamentals"
id="toc-part-1-understanding-the-fundamentals"><span
class="toc-section-number">1.1</span> Part 1: Understanding the
Fundamentals</a></li>
<li><a href="#part-2-building-the-agent"
id="toc-part-2-building-the-agent"><span
class="toc-section-number">1.2</span> Part 2: Building the
Agent</a></li>
</ul></li>
<li><a href="#the-complete-code" id="toc-the-complete-code"><span
class="toc-section-number">2</span> The Complete Code:</a></li>
<li><a href="#chapter-2-basics-of-q-learning"
id="toc-chapter-2-basics-of-q-learning"><span
class="toc-section-number">3</span> Chapter 2: Basics of
Q-Learning</a></li>
<li><a
href="#chapter-3-fundamentals-of-deep-learning-for-reinforcement-learning"
id="toc-chapter-3-fundamentals-of-deep-learning-for-reinforcement-learning"><span
class="toc-section-number">4</span> Chapter 3: Fundamentals of Deep
Learning for Reinforcement Learning</a>
<ul>
<li><a href="#overview-1" id="toc-overview-1"><span
class="toc-section-number">4.1</span> Overview</a></li>
<li><a href="#objectives-1" id="toc-objectives-1"><span
class="toc-section-number">4.2</span> Objectives</a></li>
<li><a href="#introduction-to-neural-networks"
id="toc-introduction-to-neural-networks"><span
class="toc-section-number">4.3</span> Introduction to Neural
Networks</a></li>
<li><a href="#how-neural-networks-learn"
id="toc-how-neural-networks-learn"><span
class="toc-section-number">4.4</span> How Neural Networks Learn</a></li>
<li><a href="#deep-learning-in-reinforcement-learning"
id="toc-deep-learning-in-reinforcement-learning"><span
class="toc-section-number">4.5</span> Deep Learning in Reinforcement
Learning</a></li>
<li><a href="#enhancing-gridworld-with-deep-learning"
id="toc-enhancing-gridworld-with-deep-learning"><span
class="toc-section-number">4.6</span> Enhancing Gridworld with Deep
Learning</a></li>
<li><a href="#summary-2" id="toc-summary-2"><span
class="toc-section-number">4.7</span> Summary</a></li>
<li><a href="#implementing-a-deep-q-network-dqn-with-pytorch"
id="toc-implementing-a-deep-q-network-dqn-with-pytorch"><span
class="toc-section-number">4.8</span> Implementing a Deep Q-Network
(DQN) with PyTorch</a></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1"
id="chapter-1-introduction-to-reinforcement-learning"><span
class="header-section-number">1</span> Chapter 1: Introduction to
Reinforcement Learning</h1>
<h2 data-number="1.1" id="part-1-understanding-the-fundamentals"><span
class="header-section-number">1.1</span> Part 1: Understanding the
Fundamentals</h2>
<h3 data-number="1.1.1" id="overview"><span
class="header-section-number">1.1.1</span> Overview</h3>
<p>Reinforcement learning (RL) is a paradigm in machine learning that
provides a framework for agents to learn how to behave in an environment
by performing actions and seeing the results. This chapter introduces
RL, explores its key concepts, and applies them in a simple Gridworld
environment.</p>
<h3 data-number="1.1.2" id="objectives"><span
class="header-section-number">1.1.2</span> Objectives</h3>
<ul>
<li>Understand the core principles of RL.</li>
<li>Identify RL applications in different industries.</li>
<li>Establish a foundational project in RL.</li>
</ul>
<h3 data-number="1.1.3" id="what-is-reinforcement-learning"><span
class="header-section-number">1.1.3</span> What is Reinforcement
Learning?</h3>
<p>RL involves an agent that learns to make decisions by taking actions
in an environment to maximize some notion of cumulative reward. It’s
characterized by trial and error, feedback loops, and adaptability to
changing situations.</p>
<h4 data-number="1.1.3.1" id="key-components-of-rl"><span
class="header-section-number">1.1.3.1</span> Key Components of RL</h4>
<ul>
<li><strong>Agent</strong>: The learner or decision-maker.</li>
<li><strong>Environment</strong>: Where the agent takes actions.</li>
<li><strong>Action</strong>: A set of operations that the agent can
perform.</li>
<li><strong>State</strong>: The current situation of the
environment.</li>
<li><strong>Reward</strong>: Feedback from the environment to the
agent.</li>
</ul>
<h3 data-number="1.1.4" id="why-reinforcement-learning"><span
class="header-section-number">1.1.4</span> Why Reinforcement
Learning?</h3>
<p>Reinforcement learning is unique in its approach to problem-solving,
allowing agents to learn from their own experiences rather than being
told the correct actions. This is particularly useful in complex,
unpredictable environments or when the desired behavior is difficult to
express with explicit rules.</p>
<h3 data-number="1.1.5" id="applications-of-rl"><span
class="header-section-number">1.1.5</span> Applications of RL</h3>
<ul>
<li><strong>Game playing</strong>: Achieving superhuman performance in
complex games.</li>
<li><strong>Robotics</strong>: Teaching robots to perform tasks
autonomously.</li>
<li><strong>Autonomous vehicles</strong>: Driving safely and efficiently
in dynamic environments.</li>
</ul>
<h3 data-number="1.1.6" id="project-overview-the-gridworld"><span
class="header-section-number">1.1.6</span> Project Overview: The
Gridworld</h3>
<p>The Gridworld is an introductory RL project where an agent must
navigate through a grid to reach a goal while avoiding obstacles.</p>
<h4 data-number="1.1.6.1" id="project-goal"><span
class="header-section-number">1.1.6.1</span> Project Goal</h4>
<p>Create an agent that finds the shortest path to a goal within a grid,
considering obstacles.</p>
<h4 data-number="1.1.6.2" id="key-learning"><span
class="header-section-number">1.1.6.2</span> Key Learning</h4>
<p>This project will teach you about the interaction between an agent
and its environment, the role of rewards, and how to implement these
concepts in code.</p>
<h3 data-number="1.1.7" id="coding-the-gridworld-environment"><span
class="header-section-number">1.1.7</span> Coding the Gridworld
Environment</h3>
<p>We construct a <code>Gridworld</code> class in Python to simulate the
environment for our agent.</p>
<ul>
<li>File: <code>gridworld.py</code></li>
</ul>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># gridworld.py</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Gridworld:</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, width, height, start, goal, obstacles):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">        Initializes a Gridworld object.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">        - width (int): The width of the grid.</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">        - height (int): The height of the grid.</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">        - start (tuple): The starting position of the agent as a tuple (x, y).</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">        - goal (tuple): The goal position of the agent as a tuple (x, y).</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">        - obstacles (list): A list of obstacle positions as tuples [(x1, y1), (x2, y2), ...].</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.width <span class="op">=</span> width</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.height <span class="op">=</span> height</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.start <span class="op">=</span> start</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.goal <span class="op">=</span> goal</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.obstacles <span class="op">=</span> obstacles</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reset()</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, action):</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co">        Takes an action in the environment and updates the agent&#39;s position.</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co">        - action (tuple): The action to be taken.</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co">        - new_state: The new state after taking the action.</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co">        - reward: The reward received after taking the action.</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co">        - done: A boolean indicating if the goal has been reached.</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the new position after taking the action</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        new_x <span class="op">=</span> <span class="va">self</span>.agent_position[<span class="dv">0</span>] <span class="op">+</span> action[<span class="dv">0</span>]</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        new_y <span class="op">=</span> <span class="va">self</span>.agent_position[<span class="dv">1</span>] <span class="op">+</span> action[<span class="dv">1</span>]</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check if the new position is within the grid bounds and not an obstacle</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (<span class="dv">0</span> <span class="op">&lt;=</span> new_x <span class="op">&lt;</span> <span class="va">self</span>.width) <span class="kw">and</span> (<span class="dv">0</span> <span class="op">&lt;=</span> new_y <span class="op">&lt;</span> <span class="va">self</span>.height) <span class="kw">and</span> <span class="kw">not</span> (new_x, new_y) <span class="kw">in</span> <span class="va">self</span>.obstacles:</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update the agent&#39;s position</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.agent_position <span class="op">=</span> (new_x, new_y)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check if the new position is the goal</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        done <span class="op">=</span> <span class="va">self</span>.agent_position <span class="op">==</span> <span class="va">self</span>.goal</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define the reward for reaching the goal or taking a step</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> <span class="dv">0</span> <span class="cf">if</span> done <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.get_state(), reward, done</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>):</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="co">        Resets the grid and agent position to the initial state.</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.agent_position <span class="op">=</span> <span class="va">self</span>.start</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grid <span class="op">=</span> np.zeros((<span class="va">self</span>.height, <span class="va">self</span>.width))</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> obstacle <span class="kw">in</span> <span class="va">self</span>.obstacles:</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grid[obstacle] <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grid[<span class="va">self</span>.goal] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.get_state()  <span class="co"># Return the initial state</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> render(<span class="va">self</span>):</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="co">        Renders the current state of the gridworld.</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> y <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.height):</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.width):</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> (x, y) <span class="op">==</span> <span class="va">self</span>.agent_position:</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="st">&#39;A&#39;</span>, end<span class="op">=</span><span class="st">&#39; &#39;</span>)  <span class="co"># Agent&#39;s position</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> (x, y) <span class="op">==</span> <span class="va">self</span>.goal:</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="st">&#39;G&#39;</span>, end<span class="op">=</span><span class="st">&#39; &#39;</span>)  <span class="co"># Goal position</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> (x, y) <span class="kw">in</span> <span class="va">self</span>.obstacles:</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="st">&#39;#&#39;</span>, end<span class="op">=</span><span class="st">&#39; &#39;</span>)  <span class="co"># Obstacle</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="st">&#39;.&#39;</span>, end<span class="op">=</span><span class="st">&#39; &#39;</span>)  <span class="co"># Empty cell</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>()  <span class="co"># Newline at the end of each row</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>()  <span class="co"># Extra newline for better separation between steps</span></span></code></pre></div>
<h3 data-number="1.1.8" id="summary"><span
class="header-section-number">1.1.8</span> Summary</h3>
<p>This section established the basics of RL and introduced a simple
Gridworld environment for future exploration.</p>
<h2 data-number="1.2" id="part-2-building-the-agent"><span
class="header-section-number">1.2</span> Part 2: Building the Agent</h2>
<h3 data-number="1.2.1" id="implementing-the-agent"><span
class="header-section-number">1.2.1</span> Implementing the Agent</h3>
<p>We now add an agent to interact with the Gridworld, beginning with a
basic agent that makes random moves.</p>
<h4 data-number="1.2.1.1" id="agent-and-policy"><span
class="header-section-number">1.2.1.1</span> Agent and Policy</h4>
<ul>
<li><strong>Agent</strong>: The entity that acts in the
environment.</li>
<li><strong>Policy</strong>: The decision-making strategy of the
agent.</li>
</ul>
<h4 data-number="1.2.1.2" id="the-randomagent"><span
class="header-section-number">1.2.1.2</span> The RandomAgent</h4>
<p>We implement a <code>RandomAgent</code> class, which serves as our
initial, naive agent.</p>
<ul>
<li>File: <code>gridworld_agent.py</code></li>
</ul>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># gridworld_agent.py</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RandomAgent:</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, actions):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">        Initializes a RandomAgent object.</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">        - actions (list): A list of possible actions the agent can take.</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.actions <span class="op">=</span> actions</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_action(<span class="va">self</span>, state):</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Chooses the next action at random from the list of possible actions.</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co">        - state: The current state of the agent (not used in this random policy).</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co">        A randomly selected action from the agent&#39;s list of possible actions.</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random.choice(<span class="va">self</span>.actions)</span></code></pre></div>
<h4 data-number="1.2.1.3" id="agent-actions-in-gridworld"><span
class="header-section-number">1.2.1.3</span> Agent Actions in
Gridworld</h4>
<p>The agent can move in four directions: up, down, left, and right.</p>
<h4 data-number="1.2.1.4" id="interactions-and-outcomes"><span
class="header-section-number">1.2.1.4</span> Interactions and
Outcomes</h4>
<p>We simulate the agent’s interactions with the Gridworld, using a
reward system to guide its learning.</p>
<h4 data-number="1.2.1.5" id="running-the-agent"><span
class="header-section-number">1.2.1.5</span> Running the Agent</h4>
<p>We execute the agent within the Gridworld, observing its behavior
over time.</p>
<h3 data-number="1.2.2" id="summary-and-next-steps"><span
class="header-section-number">1.2.2</span> Summary and Next Steps</h3>
<p>We introduced a simple agent to our Gridworld project. This sets the
foundation for more advanced learning algorithms to come.</p>
<hr />
<h1 data-number="2" id="the-complete-code"><span
class="header-section-number">2</span> The Complete Code:</h1>
<p>We combine the elements from Part 1 and Part 2 to run our Gridworld
simulation with the RandomAgent. The code for the agent’s interaction
with the Gridworld is in the <code>gridworld_random_agent.py</code>
file.</p>
<ul>
<li>File: <code>gridworld_random_agent.py</code></li>
</ul>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># gridworld_random_agent.py</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gridworld <span class="im">import</span> Gridworld</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gridworld_agent <span class="im">import</span> RandomAgent</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the Gridworld environment</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>gridworld <span class="op">=</span> Gridworld(width<span class="op">=</span><span class="dv">5</span>, height<span class="op">=</span><span class="dv">5</span>, start<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>), goal<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>), obstacles<span class="op">=</span>[(<span class="dv">1</span>, <span class="dv">1</span>), (<span class="dv">2</span>, <span class="dv">2</span>), (<span class="dv">3</span>, <span class="dv">3</span>)])</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the actions and create the RandomAgent</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>actions <span class="op">=</span> [(<span class="dv">0</span>, <span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">0</span>), (<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>), (<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>)]  <span class="co"># Actions corresponding to [Up, Right, Down, Left]</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> RandomAgent(actions)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>total_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the agent in the environment</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):  <span class="co"># Run for a certain number of steps or until the goal is reached</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    current_state <span class="op">=</span> gridworld.agent_position</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    action <span class="op">=</span> agent.choose_action(current_state)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    new_state, reward, done <span class="op">=</span> gridworld.step(action)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    total_reward <span class="op">+=</span> reward</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Action taken: </span><span class="sc">{</span>action<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Reward received: </span><span class="sc">{</span>reward<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Total reward: </span><span class="sc">{</span>total_reward<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    gridworld.render()</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> done:</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&quot;Goal reached!&quot;</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code></pre></div>
<h3 data-number="2.0.1" id="why-these-actions"><span
class="header-section-number">2.0.1</span> Why These Actions?</h3>
<p>We selected standard grid movement actions for simplicity and to
establish a baseline for agent behavior.</p>
<h3 data-number="2.0.2" id="following-equations-and-logic"><span
class="header-section-number">2.0.2</span> Following Equations and
Logic</h3>
<p>At this stage, the agent’s decisions are random. We’ll later
introduce more sophisticated strategies based on RL algorithms and
mathematical foundations.</p>
<h3 data-number="2.0.3" id="summary-1"><span
class="header-section-number">2.0.3</span> Summary</h3>
<p>Chapter 1 sets the stage for an exploration into reinforcement
learning, providing the fundamental concepts and a practical project to
apply these ideas. As we progress, the agents will evolve from making
random moves to employing advanced strategies informed by their
interactions with the environment.</p>
<h1 data-number="3" id="chapter-2-basics-of-q-learning"><span
class="header-section-number">3</span> Chapter 2: Basics of
Q-Learning</h1>
<h3 data-number="3.0.1" id="introduction-to-q-learning"><span
class="header-section-number">3.0.1</span> Introduction to
Q-Learning</h3>
<p>Q-Learning is a foundational model-free reinforcement learning
algorithm that enables agents to learn optimal policies for
decision-making without knowing the dynamics of the environment they
operate in. By interacting with the environment, the agent learns to
associate actions with rewards and discovers the best strategy through
trial and error.</p>
<h3 data-number="3.0.2" id="understanding-q-values"><span
class="header-section-number">3.0.2</span> Understanding Q-values</h3>
<p>At the core of Q-Learning are the Q-values, which represent the
expected cumulative reward of taking an action in a given state and
following the optimal policy thereafter. The Q-value function is denoted
by Q(s, a), where ‘s’ stands for state and ‘a’ for action.</p>
<h3 data-number="3.0.3"
id="the-q-learning-algorithm-process-and-components"><span
class="header-section-number">3.0.3</span> The Q-Learning Algorithm:
Process and Components</h3>
<p>The Q-Learning algorithm can be summarized in several key steps:</p>
<ol type="1">
<li><p>Initialize the Q-values table (Q-table) arbitrarily for all
state-action pairs.</p></li>
<li><p>Observe the current state <code>s</code>.</p></li>
<li><p>Choose an action <code>a</code> for the state <code>s</code>
based on a policy derived from the Q-values (e.g., ε-greedy).</p></li>
<li><p>Take the action <code>a</code>, and observe the outcome state
<code>s'</code> and reward <code>r</code>.</p></li>
<li><p>Update the Q-value for the state-action pair based on the
formula:</p>
<p>Q(s, a) = Q(s, a) + α * <a
href="This%20is%20the%20difference%20between%20the%20learned%20value%20and%20the%20old%20value;%20it%20is%20sometimes%20termed%20as%20the%20Temporal%20Difference%20error%20or%20simply%20TD%20error.">r
+ γ * maxQ(s’, a’) - Q(s, a)</a></p>
<p>where:</p>
<ul>
<li>α (alpha) is the learning rate (0 &lt; α ≤ 1)</li>
<li>γ (gamma) is the discount factor (0 ≤ γ &lt; 1)</li>
</ul></li>
<li><p>Set the state <code>s</code> to the new state
<code>s'</code>.</p></li>
<li><p>If the end of the episode is not reached, go back to step
3.</p></li>
<li><p>Repeat these steps for many episodes to train the agent.</p></li>
</ol>
<p>The components of the Q-learning algorithm are:</p>
<ul>
<li><strong>Q-table</strong>: A lookup table where Q-values are stored
for each state-action pair.</li>
<li><strong>Policy</strong>: A strategy that the agent employs to
determine the next action based on the Q-table.</li>
<li><strong>Learning Rate (α)</strong>: Determines how much new
information overrides old information.</li>
<li><strong>Discount Factor (γ)</strong>: Measures the importance of
future rewards over immediate ones.</li>
<li><strong>Reward (r)</strong>: The signal received from the
environment to evaluate the last action.</li>
</ul>
<p>The formula Q(s, a) = Q(s, a) + α * <a
href="This%20is%20the%20difference%20between%20the%20learned%20value%20and%20the%20old%20value;%20it%20is%20sometimes%20termed%20as%20the%20Temporal%20Difference%20error%20or%20simply%20TD%20error.">r
+ γ * maxQ(s’, a’) - Q(s, a)</a> is used to update the Q-value of the
state-action pair. Let’s break this down:</p>
<ul>
<li>Q(s, a): This is the old or previous Q-value of the pair (state,
action).</li>
<li>r: This is the immediate reward got after performing the action ‘a’
in state ‘s’.</li>
<li>maxQ(s’, a’): This is the maximum Q-value possible for the next
state ‘s’ across all possible actions. It is picked based on the current
knowledge the agent has (i.e., based on the current Q-table).</li>
<li>γ * maxQ(s’, a’): This term is the discounted maximum future reward
from the next state ‘s’. The discount factor ‘γ’ ensures the agent cares
more about immediate reward over distant or future reward; if ‘γ’ is
close to 1 the agent will consider future rewards significantly but if
‘γ’ is close to 0 the agent cares mostly about immediate rewards.</li>
<li></li>
<li>α * <a
href="This%20is%20the%20difference%20between%20the%20learned%20value%20and%20the%20old%20value;%20it%20is%20sometimes%20termed%20as%20the%20Temporal%20Difference%20error%20or%20simply%20TD%20error.">r
+ γ * maxQ(s’, a’) - Q(s, a)</a>: The learning rate ‘α’ is applied to
the TD error to moderate the update. If ‘α’ is closer to 1, learning is
faster as it depends more on the newest experience; if ‘α’ is closer to
0, learning is slower as it leverages more on cumulative past
experiences.</li>
</ul>
<p>So, this entire formula represents the idea that the Q-value for a
given state-action pair is updated by the learning rate ‘α’ times the
difference between the learned value (reward plus discounted future
value) and the old value. This guides the agent to better actions as it
navigates and learns from its environment.</p>
<h3 data-number="3.0.4" id="convergence-of-q-learning"><span
class="header-section-number">3.0.4</span> Convergence of
Q-Learning</h3>
<p>In the context of Q-Learning, convergence refers to the point at
which the Q-values stop updating significantly and remain stable. This
means that the algorithm has learned the optimal policy, that is, the
best action to take in each state to maximize its future rewards.</p>
<p>When the Q-Learning algorithm converges, the agent’s knowledge about
which actions to take in the various states is as good as it can get for
a given environment. From this point, running more training episodes
will no longer change the agent’s behavior or improve its
performance.</p>
<p>It is important to note that convergence to the optimal Q-values
assumes that proper conditions have been met. This includes assumptions
like infinite visits to each state-action pair (ensuring sufficient
exploration), appropriate setting of parameters such as the learning
rate and discount factor, among other considerations.</p>
<hr />
<h3 data-number="3.0.5" id="project-q-learning-in-a-grid-world"><span
class="header-section-number">3.0.5</span> Project: Q-Learning in a Grid
World</h3>
<p>We’ll build a Q-Learning agent to navigate a Grid World environment.
This project will solidify the concepts presented in this chapter and
provide practical experience with the algorithm.</p>
<h4 data-number="3.0.5.1"
id="setting-up-a-simple-grid-world-environment"><span
class="header-section-number">3.0.5.1</span> Setting Up a Simple Grid
World Environment</h4>
<p>We’ll use the Gridworld environment from Chapter 1, which provides
the necessary methods to simulate an agent’s interaction with the
environment.</p>
<h4 data-number="3.0.5.2" id="implementing-q-learning-algorithm"><span
class="header-section-number">3.0.5.2</span> Implementing Q-Learning
Algorithm</h4>
<p>We will implement the Q-Learning algorithm in a Python script called
<code>q_learning_agent.py</code>. The script will interface with the
Gridworld class to train the agent.</p>
<h4 data-number="3.0.5.3"
id="defining-states-and-actions-in-the-grid-world"><span
class="header-section-number">3.0.5.3</span> Defining States and Actions
in the Grid World</h4>
<p>The Grid World environment will consist of discrete states and
actions. The agent can move in four directions: up, down, left, and
right.</p>
<h4 data-number="3.0.5.4" id="experimenting-with-q-value-updates"><span
class="header-section-number">3.0.5.4</span> Experimenting with Q-Value
Updates</h4>
<p>The agent will update its Q-values based on the rewards it receives
from the environment. The goal is for the agent to learn how to reach
the goal efficiently.</p>
<hr />
<p>Now, let’s begin our project with the
<code>q_learning_agent.py</code> script.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># q_learning_agent.py</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gridworld <span class="im">import</span> Gridworld</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up logging</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>logging.basicConfig(level<span class="op">=</span>logging.INFO, <span class="bu">format</span><span class="op">=</span><span class="st">&#39;</span><span class="sc">%(asctime)s</span><span class="st"> - </span><span class="sc">%(levelname)s</span><span class="st"> - </span><span class="sc">%(message)s</span><span class="st">&#39;</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>logger <span class="op">=</span> logging.getLogger()</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the Q-learning agent</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QLearningAgent:</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, alpha, gamma, epsilon, action_space, state_space_size):</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> alpha  <span class="co"># Learning rate</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> gamma  <span class="co"># Discount factor</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon <span class="op">=</span> epsilon  <span class="co"># Exploration rate</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action_space <span class="op">=</span> action_space  <span class="co"># Available actions</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q <span class="op">=</span> np.zeros((state_space_size, <span class="bu">len</span>(action_space)))  <span class="co"># Q-value table</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_action(<span class="va">self</span>, state):</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;Choose an action using an ε-greedy policy.&quot;&quot;&quot;</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> random.random() <span class="op">&lt;</span> <span class="va">self</span>.epsilon:</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Randomly choose an index corresponding to an action</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> random.randint(<span class="dv">0</span>, <span class="bu">len</span>(<span class="va">self</span>.action_space) <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Choose the index of the action with the highest Q-value</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> np.argmax(<span class="va">self</span>.Q[state])</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_Q(<span class="va">self</span>, state, action, reward, next_state):</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;Update the Q-value for the state-action pair using NumPy for efficient computation.&quot;&quot;&quot;</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        max_future_q <span class="op">=</span> np.<span class="bu">max</span>(<span class="va">self</span>.Q[next_state])</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        current_q <span class="op">=</span> <span class="va">self</span>.Q[state, action]</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        new_q <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.alpha) <span class="op">*</span> current_q <span class="op">+</span> <span class="va">self</span>.alpha <span class="op">*</span> (reward <span class="op">+</span> <span class="va">self</span>.gamma <span class="op">*</span> max_future_q)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q[state, action] <span class="op">=</span> new_q</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, env, num_episodes):</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;Train the agent over a specified number of episodes.&quot;&quot;&quot;</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        action_space <span class="op">=</span> [(<span class="dv">0</span>, <span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">0</span>), (<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>), (<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>)]  <span class="co"># Actions: right, down, left, up</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        total_rewards <span class="op">=</span> []</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        steps_per_episode <span class="op">=</span> []</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(num_episodes):</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> env.reset()</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>            done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>            total_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>            steps <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>            <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>                action_index <span class="op">=</span> <span class="va">self</span>.choose_action(state)</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>                next_state, reward, done <span class="op">=</span> env.step(action_space[action_index])</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.update_Q(state, action_index, reward, next_state)</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>                state <span class="op">=</span> next_state</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>                total_reward <span class="op">+=</span> reward</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>                steps <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>            total_rewards.append(total_reward)</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>            steps_per_episode.append(steps)</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Log episode information</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>            logger.info(<span class="ss">f&#39;Episode </span><span class="sc">{</span>episode <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_episodes<span class="sc">}</span><span class="ss"> - &#39;</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>                        <span class="ss">f&#39;Total Reward: </span><span class="sc">{</span>total_reward<span class="sc">}</span><span class="ss">, &#39;</span></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>                        <span class="ss">f&#39;Steps: </span><span class="sc">{</span>steps<span class="sc">}</span><span class="ss">, &#39;</span></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>                        <span class="ss">f&#39;Epsilon: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>epsilon<span class="sc">:.4f}</span><span class="ss">&#39;</span>)</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># After training, log summary statistics</span></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>        logger.info(<span class="ss">f&#39;Training complete. Average reward per episode: </span><span class="sc">{</span>np<span class="sc">.</span>mean(total_rewards)<span class="sc">:.2f}</span><span class="ss">&#39;</span>)</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>        logger.info(<span class="ss">f&#39;Average steps per episode: </span><span class="sc">{</span>np<span class="sc">.</span>mean(steps_per_episode)<span class="sc">:.2f}</span><span class="ss">&#39;</span>)</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Main execution starts here</span></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&#39;__main__&#39;</span>:</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define the environment parameters</span></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>    env_params <span class="op">=</span> {</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;width&#39;</span>: <span class="dv">5</span>,</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;height&#39;</span>: <span class="dv">5</span>,</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;start&#39;</span>: (<span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;goal&#39;</span>: (<span class="dv">4</span>, <span class="dv">4</span>),</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;obstacles&#39;</span>: [(<span class="dv">1</span>, <span class="dv">1</span>), (<span class="dv">2</span>, <span class="dv">2</span>), (<span class="dv">3</span>, <span class="dv">3</span>)]</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the Gridworld environment</span></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>    env <span class="op">=</span> Gridworld(<span class="op">**</span>env_params)</span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Map the actions to indices for the Q-table</span></span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>    action_space <span class="op">=</span> [(<span class="dv">0</span>, <span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">0</span>), (<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>), (<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>)]  <span class="co"># Actions: right, down, left, up</span></span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>    action_indices <span class="op">=</span> {action: idx <span class="cf">for</span> idx, action <span class="kw">in</span> <span class="bu">enumerate</span>(action_space)}</span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the state space size</span></span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>    state_space_size <span class="op">=</span> env.width <span class="op">*</span> env.height</span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the Q-learning agent with NumPy Q-table</span></span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>    agent <span class="op">=</span> QLearningAgent(alpha<span class="op">=</span><span class="fl">0.1</span>, gamma<span class="op">=</span><span class="fl">0.9</span>, epsilon<span class="op">=</span><span class="fl">0.1</span>, action_space<span class="op">=</span>action_indices, state_space_size<span class="op">=</span>state_space_size)</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Number of episodes to train the agent</span></span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>    num_episodes <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train the agent</span></span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>    agent.train(env, num_episodes)</span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>     <span class="co"># Save the trained Q-value table to a file</span></span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>    np.savetxt(<span class="st">&#39;q_values.txt&#39;</span>, agent.Q)</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="st">&quot;Training complete. Q-values saved to q_values.txt&quot;</span>)</span></code></pre></div>
<p>This script sets up the Q-Learning agent, defines its methods, and
runs the training process. The agent is tested in the
<code>Gridworld</code> environment and its Q-values are saved to a file
after training.</p>
<p>Continuing with the remaining sections of the chapter and further
description of the code:</p>
<h3 data-number="3.0.6" id="running-the-training-process"><span
class="header-section-number">3.0.6</span> Running the Training
Process</h3>
<p>With the <code>q_learning_agent.py</code> script defined, we can now
discuss the execution of the training process. The main objective is to
run the agent through a series of episodes in the <code>Gridworld</code>
environment to learn the optimal Q-values.</p>
<p>Training begins by initializing the environment and the agent. At the
start of each episode, the environment is reset to its initial state,
and the agent initializes the Q-values for that state if they haven’t
been seen before. The agent then repeatedly chooses actions, observes
the resulting new states and rewards, and updates the Q-values until the
episode ends (either the goal is reached or a terminal condition is
met).</p>
<p>The <code>train</code> method encapsulates this process, logging the
completion of each episode to help us monitor the agent’s progress.
After training for the specified number of episodes, the learned
Q-values are saved to a file. This data can be used for analysis or to
initialize the agent at a later time for further learning or policy
execution.</p>
<h3 data-number="3.0.7" id="analyzing-the-results"><span
class="header-section-number">3.0.7</span> Analyzing the Results</h3>
<p>After training, it’s beneficial to analyze the results. You can do
this by looking at the Q-values to understand what the agent has
learned. Checking the convergence of Q-values can indicate whether the
agent has learned a stable policy.</p>
<p>Another approach is to visualize the agent’s behavior in the
<code>Gridworld</code> by running it with the learned policy and
observing its actions. This can be done by modifying the
<code>train</code> method to include a rendering of the environment
after each action or by creating a separate method for policy execution
where the agent only selects the best-known action without further
exploration.</p>
<h3 data-number="3.0.8" id="considerations-for-improving-learning"><span
class="header-section-number">3.0.8</span> Considerations for Improving
Learning</h3>
<p>Several factors can influence the effectiveness of Q-Learning: -
<strong>Learning Rate (α):</strong> Determines how much new information
overrides old information. A smaller α makes the learning updates more
conservative. - <strong>Discount Factor (γ):</strong> Reflects the
importance of future rewards. A higher γ values future rewards more,
while a lower γ results in a more myopic policy. - <strong>Exploration
Rate (ε):</strong> Balances exploration and exploitation. Initially, a
higher ε encourages the agent to explore the environment. Over time,
gradually reducing ε (known as epsilon decay) can lead to better
exploitation of the learned values. - <strong>Initial Q-values:</strong>
Setting initial Q-values to optimistic estimates can encourage
exploration. This is known as optimistic initialization.</p>
<h3 data-number="3.0.9" id="conclusion-and-next-steps"><span
class="header-section-number">3.0.9</span> Conclusion and Next
Steps</h3>
<p>By implementing the Q-Learning algorithm in a practical example such
as the <code>Gridworld</code>, we have gained a deeper understanding of
how agents learn from their environment to make optimal decisions. With
the foundation built in this chapter, we can extend our knowledge to
more complex reinforcement learning algorithms and problems.</p>
<p>As a next step, consider experimenting with different parameters (α,
γ, and ε), introducing variability in rewards, or increasing the
complexity of the <code>Gridworld</code>. You can also implement
enhancements to the Q-Learning algorithm, such as using experience
replay or function approximation techniques for larger state spaces.</p>
<p>With the project and the chapter content completed, you now have a
solid understanding of Q-Learning and the know-how to implement it from
scratch. The next chapters will build on this knowledge and introduce
you to advanced topics in reinforcement learning.</p>
<hr />
<p>This concludes the full chapter and project on the basics of
Q-Learning. The chapter has introduced the key concepts, algorithmic
components, and practical application in a simple Grid World
environment. The project has provided a hands-on experience with coding
a Q-Learning agent and running it to learn a policy for navigating the
Grid World.</p>
<h1 data-number="4"
id="chapter-3-fundamentals-of-deep-learning-for-reinforcement-learning"><span
class="header-section-number">4</span> Chapter 3: Fundamentals of Deep
Learning for Reinforcement Learning</h1>
<h2 data-number="4.1" id="overview-1"><span
class="header-section-number">4.1</span> Overview</h2>
<p>This chapter delves into the synergy between deep learning and
reinforcement learning, leading to the burgeoning field of deep
reinforcement learning (DRL). We aim to elucidate the essential
principles of deep learning and show how neural networks, when paired
with RL, can effectively address intricate decision-making
challenges.</p>
<h2 data-number="4.2" id="objectives-1"><span
class="header-section-number">4.2</span> Objectives</h2>
<ul>
<li>Understand the fundamental concepts of deep learning, with a focus
on neural networks.</li>
<li>Learn how deep learning can be integrated with RL algorithms to
improve their performance.</li>
<li>Prepare to implement a deep Q-network (DQN) in the Gridworld
project.</li>
</ul>
<h2 data-number="4.3" id="introduction-to-neural-networks"><span
class="header-section-number">4.3</span> Introduction to Neural
Networks</h2>
<p>Neural networks are designed to mimic the workings of the human brain
and are made up of interconnected units called neurons. These networks
have the capacity to learn from data, enabling them to perform tasks
such as predictions or decision-making without the need for explicit
programming.</p>
<h3 data-number="4.3.1" id="key-concepts-in-neural-networks"><span
class="header-section-number">4.3.1</span> Key Concepts in Neural
Networks</h3>
<ul>
<li><strong>Neurons</strong>: The building blocks of neural networks
that take input, process it using a weighted sum followed by an
activation function, and produce an output.</li>
<li><strong>Layers</strong>: A neural network comprises different layers
of neurons, which include the input layer, one or more hidden layers,
and the output layer.</li>
<li><strong>Weights and Biases</strong>: Parameters of the model that
adjust during training to make the neural network’s predictions as
accurate as possible.</li>
</ul>
<h2 data-number="4.4" id="how-neural-networks-learn"><span
class="header-section-number">4.4</span> How Neural Networks Learn</h2>
<p>The process of learning in neural networks involves adjusting their
weights and biases. The goal is to reduce the discrepancy between the
model’s predictions and the actual target values.</p>
<ul>
<li><strong>Forward Propagation</strong>: This is the process where
input data is passed through the network to generate output
predictions.</li>
<li><strong>Loss Functions</strong>: These are measures used to assess
the deviation between the network’s predictions and the true
outcomes.</li>
<li><strong>Backpropagation</strong>: A key algorithm in neural network
training, backpropagation calculates the gradient of the loss function
with respect to each weight and bias, using these gradients to update
the parameters via gradient descent.</li>
</ul>
<h2 data-number="4.5" id="deep-learning-in-reinforcement-learning"><span
class="header-section-number">4.5</span> Deep Learning in Reinforcement
Learning</h2>
<p>Deep learning equips RL with powerful function approximation
capabilities, essential for dealing with high-dimensional state or
action spaces.</p>
<h3 data-number="4.5.1" id="deep-reinforcement-learning"><span
class="header-section-number">4.5.1</span> Deep Reinforcement
Learning</h3>
<ul>
<li><strong>Function Approximation</strong>: The application of deep
neural networks to estimate value functions or policy functions.</li>
<li><strong>Advantages of Deep RL</strong>: These include the ability to
process complex inputs, generalize across different states, and discern
underlying patterns in the data.</li>
</ul>
<h2 data-number="4.6" id="enhancing-gridworld-with-deep-learning"><span
class="header-section-number">4.6</span> Enhancing Gridworld with Deep
Learning</h2>
<p>To integrate deep learning into our Gridworld example, we will employ
a deep Q-network (DQN). A DQN utilizes a neural network to estimate the
Q-value function, a central concept in RL that predicts the quality of
actions taken in various states.</p>
<ul>
<li><strong>Project Update</strong>: We will implement a DQN to
substitute the Q-table with a neural network that predicts Q-values for
Gridworld.</li>
<li><strong>Implementation Steps</strong>: We outline the necessary
modifications to the existing Gridworld project to incorporate a
DQN.</li>
</ul>
<h2 data-number="4.7" id="summary-2"><span
class="header-section-number">4.7</span> Summary</h2>
<p>We recap the deep learning concepts introduced in this chapter and
discuss their relevance to RL. This provides the foundation for a
hands-on implementation of a DQN in the following sections.</p>
<hr />
<h2 data-number="4.8"
id="implementing-a-deep-q-network-dqn-with-pytorch"><span
class="header-section-number">4.8</span> Implementing a Deep Q-Network
(DQN) with PyTorch</h2>
<p>We use PyTorch to construct our DQN. The following parts detail each
section of the code.</p>
<h3 data-number="4.8.1" id="defining-the-network-architecture"><span
class="header-section-number">4.8.1</span> Defining the Network
Architecture</h3>
<p>We start by defining our neural network:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the neural network architecture for Deep Q-Learning</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleMLP(nn.Module):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Constructor for the neural network</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, output_size, hidden_size<span class="op">=</span><span class="dv">64</span>):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SimpleMLP, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First fully connected layer from input_size to hidden_size</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, hidden_size)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Second fully connected layer, hidden to hidden</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(hidden_size, hidden_size)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Third fully connected layer from hidden_size to output_size</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(hidden_size, output_size)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass definition for the neural network</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply ReLU activation function after first layer</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply ReLU activation function after second layer</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output layer, no activation function</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.fc3(x)</span></code></pre></div>
<p>The <code>SimpleMLP</code> class represents a multilayer perceptron
with layers <code>fc1</code>, <code>fc2</code>, and
<code>fc3</code>.</p>
<h3 data-number="4.8.2" id="dqn-agent-class"><span
class="header-section-number">4.8.2</span> DQN Agent Class</h3>
<p>Next, we have the <code>DQNAgent</code> class, controlling action
selection and learning from interactions with the environment:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DQNAgent:</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Constructor for the DQN agent</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, output_size, hidden_size<span class="op">=</span><span class="dv">64</span>):</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The neural network model</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> SimpleMLP(input_size, output_size, hidden_size)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Epsilon for the epsilon-greedy policy (initially set to 1 for full exploration)</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Minimum value that epsilon can decay to over time</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon_min <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Rate at which epsilon is decayed over time</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon_decay <span class="op">=</span> <span class="fl">0.995</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># List to hold past experiences for replay</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.memory <span class="op">=</span> []</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Method to decide an action based on the current state</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> act(<span class="va">self</span>, state):</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check if we should take a random action (exploration)</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.random.rand() <span class="op">&lt;=</span> <span class="va">self</span>.epsilon:</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Return a random action within the action space size</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> random.randrange(output_size)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If not exploring, process the state through the DQN to get the action values</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First, ensure state is a PyTorch tensor</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(state, torch.Tensor):</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> torch.from_numpy(state).<span class="bu">float</span>().unsqueeze(<span class="dv">0</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass through the network to get action values</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>        action_values <span class="op">=</span> <span class="va">self</span>.model(state)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return the action with the highest value</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.argmax(action_values.detach().numpy())</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Method to store experiences in replay memory</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> remember(<span class="va">self</span>, state, action, reward, next_state, done):</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Append the experience as a tuple to the memory list</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.memory.append((state, action, reward, next_state, done))</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Method to decay epsilon over time for less exploration</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_epsilon(<span class="va">self</span>):</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.epsilon <span class="op">&gt;</span> <span class="va">self</span>.epsilon_min:</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.epsilon <span class="op">*=</span> <span class="va">self</span>.epsilon_decay</span></code></pre></div>
<p>Here, the agent is defined with methods like <code>act</code> for
decision-making and constructors for agent properties.</p>
<h3 data-number="4.8.3" id="training-the-model"><span
class="header-section-number">4.8.3</span> Training the Model</h3>
<p>The <code>train_model</code> function updates the network’s
parameters using experience replay:</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model based on a batch of experience</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(agent, optimizer, batch_size, gamma):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check that there are enough experiences in memory to sample a batch</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(agent.memory) <span class="op">&lt;</span> batch_size:</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample a minibatch of experiences from memory</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    minibatch <span class="op">=</span> random.sample(agent.memory, batch_size)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Unpack the experiences</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    states, actions, rewards, next_states, dones <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>minibatch)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert experience components to PyTorch tensors</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    states <span class="op">=</span> torch.from_numpy(np.vstack(states)).<span class="bu">float</span>()</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    actions <span class="op">=</span> torch.from_numpy(np.vstack(actions)).<span class="bu">long</span>()</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    rewards <span class="op">=</span> torch.from_numpy(np.vstack(rewards)).<span class="bu">float</span>()</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    next_states <span class="op">=</span> torch.from_numpy(np.vstack(next_states)).<span class="bu">float</span>()</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    dones <span class="op">=</span> torch.from_numpy(np.vstack(dones).astype(np.uint8)).<span class="bu">float</span>()</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the expected Q values from the neural network</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    Q_expected <span class="op">=</span> agent.model(states).gather(<span class="dv">1</span>, actions)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the Q value for the next states and get the max Q value for each next state</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    Q_targets_next <span class="op">=</span> agent.model(next_states).detach().<span class="bu">max</span>(<span class="dv">1</span>)[<span class="dv">0</span>].unsqueeze(<span class="dv">1</span>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the target Q values for the current states using the Bellman equation</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    Q_targets <span class="op">=</span> rewards <span class="op">+</span> (gamma <span class="op">*</span> Q_targets_next <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> dones))</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the loss between the expected Q values and the target Q values</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.mse_loss(Q_expected, Q_targets)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backpropagate the loss, update the network weights</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span></code></pre></div>
<p>This function calculates the loss between the predicted and target
Q-values and performs backpropagation to update the network weights.</p>
<h3 data-number="4.8.4" id="logging-agents-performance"><span
class="header-section-number">4.8.4</span> Logging Agent’s
Performance</h3>
<p>Finally, a simple logging function keeps track of the agent’s
learning progress:</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Log the performance metrics</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_performance(episode, total_reward, steps, epsilon):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print out the episode number, total reward, number of steps and the epsilon value</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Episode: </span><span class="sc">{</span>episode<span class="sc">}</span><span class="ss">, Total Reward: </span><span class="sc">{</span>total_reward<span class="sc">}</span><span class="ss">, Steps: </span><span class="sc">{</span>steps<span class="sc">}</span><span class="ss">, Epsilon: </span><span class="sc">{</span>epsilon<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<p>This utility function prints information about the episode number,
total rewards, steps, and the epsilon value.</p>
<h3 data-number="4.8.5" id="example-code-execution"><span
class="header-section-number">4.8.5</span> Example Code Execution</h3>
<p>The main function demonstrates initializing the environment, agent,
training, and logging:</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the Gridworld environment</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    gridworld <span class="op">=</span> Gridworld(width<span class="op">=</span><span class="dv">5</span>, height<span class="op">=</span><span class="dv">5</span>, start<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>), goal<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>), obstacles<span class="op">=</span>[(<span class="dv">1</span>, <span class="dv">1</span>), (<span class="dv">2</span>, <span class="dv">2</span>), (<span class="dv">3</span>, <span class="dv">3</span>)])</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define action mapping that maps action indices to movements</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    action_mapping <span class="op">=</span> [(<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">0</span>), (<span class="dv">0</span>, <span class="dv">1</span>), (<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>)]  <span class="co"># Up, Right, Down, Left</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the input size based on the environment&#39;s state space</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    input_size <span class="op">=</span> gridworld.width <span class="op">*</span> gridworld.height</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The output size is the number of possible actions</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    output_size <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the DQN agent</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    agent <span class="op">=</span> DQNAgent(input_size, output_size)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define an optimizer for the neural network (Adam optimizer with a learning rate of 0.001)</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(agent.model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Batch size for experience replay</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Discount factor for future rewards</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    gamma <span class="op">=</span> <span class="fl">0.99</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Function to convert a state to a tensor for neural network input</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> state_to_tensor(state, grid_size<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a one-hot encoded vector for the state</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        state_vector <span class="op">=</span> torch.zeros(grid_size <span class="op">*</span> grid_size, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        state_vector[state] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> state_vector</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Main training loop</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reset the environment at the start of each episode</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> gridworld.reset()</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize total reward and steps counter</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        total_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        steps <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop for each step in the episode</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Convert state to tensor format</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>            state_vector <span class="op">=</span> state_to_tensor(state)</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Select an action using the DQN agent&#39;s policy</span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> agent.act(state_vector)</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Take the action in the environment and observe the next state and reward</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>            next_state, reward, done <span class="op">=</span> gridworld.step(action_mapping[action])</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Convert the next state to tensor format</span></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>            next_state_vector <span class="op">=</span> state_to_tensor(next_state)</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Remember the experience</span></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>            agent.remember(state_vector, action, reward, next_state_vector, done)</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Move to the next state</span></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> next_state</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update the total reward</span></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>            total_reward <span class="op">+=</span> reward</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Increment the step counter</span></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>            steps <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Train the model with experiences from memory</span></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>            train_model(agent, optimizer, batch_size, gamma)</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># After the episode, decay epsilon for less exploration in future episodes</span></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>        agent.update_epsilon()</span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Log the performance metrics for the episode</span></span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>        log_performance(episode, total_reward, steps, agent.epsilon)</span></code></pre></div>
<p>Within each episode, the agent selects actions, updates its Q-values,
and logs its performance.</p>
<hr />
<p>In this python application, we have outlined the creation of a simple
feedforward neural network model using PyTorch for approximating
Q-values in the Gridworld environment. The code includes a DQN agent
that selects actions using an epsilon-greedy policy and stores
experiences in a replay buffer. The <code>train_model</code> function
updates the neural network’s weights using sampled experiences to reduce
the loss between predicted and target Q-values.</p>
<p>Please note that this example is a simplified version of DQN. For
practical and more complex scenarios, additional mechanisms like
experience replay buffers with more sophisticated sampling techniques
and separate target networks to stabilize the Q-value predictions are
recommended.</p>
</body>
</html>
