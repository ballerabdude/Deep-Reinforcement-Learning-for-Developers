<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Abdul Hagi" />
  <title>Deep Reinforcement Learning for Developers</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="css/html.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Deep Reinforcement Learning for Developers</h1>
<p class="author">Abdul Hagi</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#chapter-1-introduction-to-reinforcement-learning"
id="toc-chapter-1-introduction-to-reinforcement-learning"><span
class="toc-section-number">1</span> Chapter 1: Introduction to
Reinforcement Learning</a>
<ul>
<li><a href="#part-1-understanding-the-fundamentals"
id="toc-part-1-understanding-the-fundamentals"><span
class="toc-section-number">1.1</span> Part 1: Understanding the
Fundamentals</a></li>
<li><a href="#part-2-building-the-agent"
id="toc-part-2-building-the-agent"><span
class="toc-section-number">1.2</span> Part 2: Building the
Agent</a></li>
<li><a href="#part-3-the-complete-code"
id="toc-part-3-the-complete-code"><span
class="toc-section-number">1.3</span> Part 3: The Complete
Code:</a></li>
</ul></li>
<li><a href="#chapter-2-basics-of-q-learning"
id="toc-chapter-2-basics-of-q-learning"><span
class="toc-section-number">2</span> Chapter 2: Basics of
Q-Learning</a></li>
<li><a
href="#chapter-3-fundamentals-of-deep-learning-for-reinforcement-learning"
id="toc-chapter-3-fundamentals-of-deep-learning-for-reinforcement-learning"><span
class="toc-section-number">3</span> Chapter 3: Fundamentals of Deep
Learning for Reinforcement Learning</a>
<ul>
<li><a href="#overview-1" id="toc-overview-1"><span
class="toc-section-number">3.1</span> Overview</a></li>
<li><a href="#objectives-1" id="toc-objectives-1"><span
class="toc-section-number">3.2</span> Objectives</a></li>
<li><a href="#introduction-to-neural-networks"
id="toc-introduction-to-neural-networks"><span
class="toc-section-number">3.3</span> Introduction to Neural
Networks</a></li>
<li><a href="#how-neural-networks-learn"
id="toc-how-neural-networks-learn"><span
class="toc-section-number">3.4</span> How Neural Networks Learn</a></li>
<li><a href="#deep-learning-in-reinforcement-learning"
id="toc-deep-learning-in-reinforcement-learning"><span
class="toc-section-number">3.5</span> Deep Learning in Reinforcement
Learning</a></li>
<li><a href="#enhancing-gridworld-with-deep-learning"
id="toc-enhancing-gridworld-with-deep-learning"><span
class="toc-section-number">3.6</span> Enhancing Gridworld with Deep
Learning</a></li>
<li><a href="#summary-2" id="toc-summary-2"><span
class="toc-section-number">3.7</span> Summary</a></li>
<li><a href="#implementing-a-deep-q-network-dqn-with-pytorch"
id="toc-implementing-a-deep-q-network-dqn-with-pytorch"><span
class="toc-section-number">3.8</span> Implementing a Deep Q-Network
(DQN) with PyTorch</a></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1"
id="chapter-1-introduction-to-reinforcement-learning"><span
class="header-section-number">1</span> Chapter 1: Introduction to
Reinforcement Learning</h1>
<h2 data-number="1.1" id="part-1-understanding-the-fundamentals"><span
class="header-section-number">1.1</span> Part 1: Understanding the
Fundamentals</h2>
<h3 data-number="1.1.1" id="overview"><span
class="header-section-number">1.1.1</span> Overview</h3>
<p>Reinforcement learning (RL) is a paradigm in machine learning that
provides a framework for agents to learn how to behave in an environment
by performing actions and seeing the results. This chapter introduces
RL, explores its key concepts, and applies them in a simple Gridworld
environment.</p>
<h3 data-number="1.1.2" id="objectives"><span
class="header-section-number">1.1.2</span> Objectives</h3>
<ul>
<li>Understand the core principles of RL.</li>
<li>Identify RL applications in different industries.</li>
<li>Establish a foundational project in RL.</li>
</ul>
<h3 data-number="1.1.3" id="what-is-reinforcement-learning"><span
class="header-section-number">1.1.3</span> What is Reinforcement
Learning?</h3>
<p>RL involves an agent that learns to make decisions by taking actions
in an environment to maximize some notion of cumulative reward. It’s
characterized by trial and error, feedback loops, and adaptability to
changing situations.</p>
<h4 data-number="1.1.3.1" id="key-components-of-rl"><span
class="header-section-number">1.1.3.1</span> Key Components of RL</h4>
<ul>
<li><strong>Agent</strong>: The learner or decision-maker.</li>
<li><strong>Environment</strong>: Where the agent takes actions.</li>
<li><strong>Action</strong>: A set of operations that the agent can
perform.</li>
<li><strong>State</strong>: The current situation of the
environment.</li>
<li><strong>Reward</strong>: Feedback from the environment to the
agent.</li>
<li><strong>Policy</strong>: The strategy that the agent employs to
determine the next action based on the current state.</li>
<li><strong>Value Function</strong>: A function that estimates how good
it is for the agent to be in a given state or how good an action is,
considering future rewards.</li>
<li><strong>Model</strong>: The agent’s representation of the
environment, which predicts the next state and reward for each
action.</li>
</ul>
<h3 data-number="1.1.4"
id="expanded-key-components-of-reinforcement-learning-for-developers"><span
class="header-section-number">1.1.4</span> Expanded Key Components of
Reinforcement Learning for Developers</h3>
<p>Understanding Reinforcement Learning (RL) requires a good grasp of
its foundational elements. Here’s a more detailed look at each key
component of RL, tailored to give developers more context and
clarity.</p>
<h4 data-number="1.1.4.1" id="agent"><span
class="header-section-number">1.1.4.1</span> Agent</h4>
<p>In the context of software, the agent is the autonomous program or
entity you create that makes decisions. This is your RL model. As a
developer, you’ll design the agent to interact with a given environment,
deciding which actions to take based on input data and its current
strategy, known as policy. The agent’s design is crucial, as it will
determine how effectively the agent can learn and accomplish its
goals.</p>
<h4 data-number="1.1.4.2" id="environment"><span
class="header-section-number">1.1.4.2</span> Environment</h4>
<p>The environment is the world in which the agent operates. It can be a
real-world setting or a simulated one. For developers, creating or
choosing the right environment is essential because the agent learns
exclusively from interacting with it. The environment provides state
information to the agent and receives actions from it, influencing how
the agent must behave to achieve its objectives.</p>
<h4 data-number="1.1.4.3" id="action"><span
class="header-section-number">1.1.4.3</span> Action</h4>
<p>Actions are the set of operations or moves the agent can perform
within the environment. When writing code for RL, you’ll need to define
what actions are possible—such as moving left, buying stock, or turning
off a switch—often represented as a finite set of choices. The agent
selects actions in a bid to achieve the greatest cumulative reward over
time.</p>
<h4 data-number="1.1.4.4" id="state"><span
class="header-section-number">1.1.4.4</span> State</h4>
<p>The state is a description of the current situation or configuration
of the environment. For developers, this translates to the data
structure or object that represents the environment at a given time.
Defining the state space is crucial, as it influences the complexity of
the learning task—the more states there are, the more scenarios the
agent has to consider.</p>
<h4 data-number="1.1.4.5" id="reward"><span
class="header-section-number">1.1.4.5</span> Reward</h4>
<p>Rewards are feedback from the environment that evaluates the agent’s
actions. Positive rewards reinforce good actions, while negative rewards
discourage bad ones. As a developer, you’ll need to code the reward
mechanism that provides this feedback to the agent. The reward structure
you define plays a major role in shaping the agent’s behavior—over time,
the agent learns to take actions that maximize the cumulative reward it
receives.</p>
<h4 data-number="1.1.4.6" id="policy"><span
class="header-section-number">1.1.4.6</span> Policy</h4>
<p>The policy is the algorithm or strategy that the agent uses to decide
which action to take in a given state. It’s essentially the
decision-making function, which you’ll need to program. In RL, policies
can range from simple rules to complex neural networks.</p>
<h4 data-number="1.1.4.7" id="value-function"><span
class="header-section-number">1.1.4.7</span> Value Function</h4>
<p>This is a prediction of the expected cumulative reward that can be
gained from a particular state or state-action pair, guiding the agent
toward long-term success. When writing RL code, you’ll need to implement
mechanisms for estimating these values, which are critical in many RL
algorithms for deciding the most rewarding paths to take.</p>
<h4 data-number="1.1.4.8" id="model"><span
class="header-section-number">1.1.4.8</span> Model</h4>
<p>In some RL methods, particularly model-based approaches, you must
also define a model that predicts how the environment will respond to
the agent’s actions—that is, how the state transitions and what rewards
are given. This can be seen as creating a simulation within which your
agent can plan ahead.</p>
<p>When developing RL applications, the code you write will incorporate
these crucial elements. By carefully building and integrating the agent,
environment, actions, states, rewards, policies, value functions, and
(if used) models, you can construct an RL system capable of learning
optimal behaviors for a wide range of problems. Remember, these
components are interdependent: changes in one can significantly affect
the performance of others, so careful consideration and iterative
refinement are key to successful RL development.</p>
<h3 data-number="1.1.5" id="why-reinforcement-learning"><span
class="header-section-number">1.1.5</span> Why Reinforcement
Learning?</h3>
<p>Reinforcement learning is unique in its approach to problem-solving,
allowing agents to learn from their own experiences rather than being
told the correct actions. This is particularly useful in complex,
unpredictable environments or when the desired behavior is difficult to
express with explicit rules.</p>
<h3 data-number="1.1.6" id="applications-of-rl"><span
class="header-section-number">1.1.6</span> Applications of RL</h3>
<p>Reinforcement learning has found applications in several fields, and
its versatility is one of its most compelling features. Here is a list,
along with how RL is implemented in these domains:</p>
<ul>
<li><p><strong>Game playing</strong>: RL agents are trained to play
complex games, such as Go or Chess, by repeatedly playing against
themselves or simulated opponents, learning strategies that maximize
their chances of winning.</p></li>
<li><p><strong>Robotics</strong>: Robots use RL to learn tasks such as
walking, grasping, or navigation by trying different motions and
receiving feedback based on their success.</p></li>
<li><p><strong>Autonomous vehicles</strong>: Self-driving cars and
drones use RL to learn how to navigate and respond to dynamic conditions
by simulating various scenarios and optimizing decisions to ensure
safety and efficiency.</p></li>
<li><p><strong>Healthcare</strong>: RL algorithms analyze patient data
to develop personalized treatment and manage hospital resources by
predicting patient flows and optimizing scheduling and
logistics.</p></li>
<li><p><strong>Finance</strong>: RL models market dynamics and automates
trading strategies, adjusting in real-time to market changes and
learning strategies that maximize long-term returns.</p></li>
<li><p><strong>Energy</strong>: Smart grids employ RL to predict demand
patterns and optimize energy distribution and consumption, resulting in
cost savings and efficiency improvements.</p></li>
</ul>
<p>These applications showcase the potential of RL to automate and
enhance decision-making processes across numerous fields, leading to
smarter and more efficient systems. By understanding how to establish
the right environment and rewards, developers can create RL solutions
that continuously learn and adapt to achieve desired outcomes.</p>
<h3 data-number="1.1.7"
id="demystifying-the-math-in-reinforcement-learning"><span
class="header-section-number">1.1.7</span> Demystifying the Math in
Reinforcement Learning</h3>
<p>One common concern among developers new to reinforcement learning is
the mathematical complexity that underlies many of the algorithms. While
a deep understanding of the math can be beneficial, especially for
research and advanced applications, it is not strictly necessary to get
started with practical RL projects. The key is to focus on the concepts
and how they can be applied.</p>
<p>In this book, we aim to strike a balance between theory and practice.
We will introduce mathematical concepts as necessary, but our primary
goal is to empower you to implement and experiment with RL algorithms.
Many of the mathematical details can be abstracted away by using modern
machine learning frameworks, which handle the heavy lifting while you
concentrate on designing the RL environment and tweaking the parameters
of the learning process.</p>
<p>Remember, complex math is a tool, not a barrier, in RL. As you become
more comfortable with the algorithms and their implementations, the
underlying math will become more intuitive. For the purpose of this
chapter, we encourage you to embrace the practical aspects of RL and
view the math as a roadmap, not a roadblock.</p>
<h3 data-number="1.1.8" id="project-overview-the-gridworld"><span
class="header-section-number">1.1.8</span> Project Overview: The
Gridworld</h3>
<p>The Gridworld is an introductory RL project where an agent must
navigate through a grid to reach a goal while avoiding obstacles.</p>
<h4 data-number="1.1.8.1" id="project-goal"><span
class="header-section-number">1.1.8.1</span> Project Goal</h4>
<p>Create an agent that finds the shortest path to a goal within a grid,
considering obstacles.</p>
<h4 data-number="1.1.8.2" id="key-learning"><span
class="header-section-number">1.1.8.2</span> Key Learning</h4>
<p>This project will teach you about the interaction between an agent
and its environment, the role of rewards, and how to implement these
concepts in code.</p>
<h3 data-number="1.1.9" id="coding-the-gridworld-environment"><span
class="header-section-number">1.1.9</span> Coding the Gridworld
Environment</h3>
<p>We construct a Gridworld class in Python to simulate the environment
for our agent.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># gridworld.py</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Gridworld:</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, width, height, start, goal, obstacles):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">        Initializes a Gridworld object.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.width <span class="op">=</span> width  <span class="co"># Set the width of the grid</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.height <span class="op">=</span> height  <span class="co"># Set the height of the grid</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.start <span class="op">=</span> start  <span class="co"># Set the starting position of the agent</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.goal <span class="op">=</span> goal  <span class="co"># Set the goal position</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.obstacles <span class="op">=</span> obstacles  <span class="co"># Set the obstacle positions</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.agent_position <span class="op">=</span> start  <span class="co"># Initialize the agent&#39;s position</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grid <span class="op">=</span> np.zeros((<span class="va">self</span>.height, <span class="va">self</span>.width))  <span class="co"># Initialize the grid as a 2D numpy array</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reset()</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_state(<span class="va">self</span>):</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns a unique state index for the agent&#39;s current position.</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co">        The method converts the 2D coordinates (x, y) of the agent&#39;s position </span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co">        into a single number that uniquely identifies each possible state in the grid.</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co">        This is done by using the formula &#39;y * self.width + x&#39;, which maps the 2D </span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co">        coordinates to a unique 1D index. This representation is useful in algorithms </span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co">        that require state representation as a single number, like many reinforcement </span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co">        learning algorithms.</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">=</span> <span class="va">self</span>.agent_position</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y <span class="op">*</span> <span class="va">self</span>.width <span class="op">+</span> x</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, action):</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="co">        Takes an action in the environment and updates the agent&#39;s position.</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate potential new position after the action</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        new_x <span class="op">=</span> <span class="va">self</span>.agent_position[<span class="dv">0</span>] <span class="op">+</span> action[<span class="dv">0</span>]</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        new_y <span class="op">=</span> <span class="va">self</span>.agent_position[<span class="dv">1</span>] <span class="op">+</span> action[<span class="dv">1</span>]</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check if the new position is within the grid boundaries and not an obstacle</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The new position must be within the range of 0 and the grid&#39;s width for the x-coordinate,</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># and within the range of 0 and the grid&#39;s height for the y-coordinate.</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Additionally, the new position should not be one of the predefined obstacles.</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (<span class="dv">0</span> <span class="op">&lt;=</span> new_x <span class="op">&lt;</span> <span class="va">self</span>.width) <span class="kw">and</span> (<span class="dv">0</span> <span class="op">&lt;=</span> new_y <span class="op">&lt;</span> <span class="va">self</span>.height) <span class="kw">and</span> <span class="kw">not</span> (new_x, new_y) <span class="kw">in</span> <span class="va">self</span>.obstacles:</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.agent_position <span class="op">=</span> (new_x, new_y)  <span class="co"># Update agent&#39;s position</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check if the agent has reached the goal</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        done <span class="op">=</span> <span class="va">self</span>.agent_position <span class="op">==</span> <span class="va">self</span>.goal</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reward logic: 0 if goal is reached, otherwise -1. Negative rewards are used to </span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># penalize certain actions or states, encouraging the agent to reach the goal efficiently.</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> <span class="dv">0</span> <span class="cf">if</span> done <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.get_state(), reward, done</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>):</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="co">        Resets the grid and agent position to the initial state.</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.agent_position <span class="op">=</span> <span class="va">self</span>.start  <span class="co"># Reset agent to the start position</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grid <span class="op">=</span> np.zeros((<span class="va">self</span>.height, <span class="va">self</span>.width))  <span class="co"># Reset the grid</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> obstacle <span class="kw">in</span> <span class="va">self</span>.obstacles:</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grid[obstacle] <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>  <span class="co"># Mark obstacles in the grid</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grid[<span class="va">self</span>.goal] <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Mark the goal in the grid</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.get_state()</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> render(<span class="va">self</span>):</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="co">        Renders the current state of the gridworld.</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> y <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.height):</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.width):</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Print symbols for agent, goal, obstacles, or empty space</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> (x, y) <span class="op">==</span> <span class="va">self</span>.agent_position:</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="st">&#39;A&#39;</span>, end<span class="op">=</span><span class="st">&#39; &#39;</span>)  <span class="co"># Agent&#39;s current position</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> (x, y) <span class="op">==</span> <span class="va">self</span>.goal:</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="st">&#39;G&#39;</span>, end<span class="op">=</span><span class="st">&#39; &#39;</span>)  <span class="co"># Goal position</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> (x, y) <span class="kw">in</span> <span class="va">self</span>.obstacles:</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="st">&#39;#&#39;</span>, end<span class="op">=</span><span class="st">&#39; &#39;</span>)  <span class="co"># Obstacle position</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="st">&#39;.&#39;</span>, end<span class="op">=</span><span class="st">&#39; &#39;</span>)  <span class="co"># Empty cell</span></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>()  <span class="co"># New line after each row</span></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>()  <span class="co"># Additional new line for separation</span></span></code></pre></div>
<h3 data-number="1.1.10" id="summary"><span
class="header-section-number">1.1.10</span> Summary</h3>
<p>This section established the basics of RL and introduced a simple
Gridworld environment for future exploration. Feel free to add the
following at the end of the file to visualize a random Gridworld:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>Gridworld(width<span class="op">=</span><span class="dv">5</span>, height<span class="op">=</span><span class="dv">5</span>, start<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>), goal<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>), obstacles<span class="op">=</span>[(<span class="dv">1</span>, <span class="dv">1</span>), (<span class="dv">2</span>, <span class="dv">2</span>), (<span class="dv">3</span>, <span class="dv">3</span>)]).render()</span></code></pre></div>
<h2 data-number="1.2" id="part-2-building-the-agent"><span
class="header-section-number">1.2</span> Part 2: Building the Agent</h2>
<h3 data-number="1.2.1" id="implementing-the-agent"><span
class="header-section-number">1.2.1</span> Implementing the Agent</h3>
<p>We now add an agent to interact with the Gridworld, beginning with a
basic agent that makes random moves.</p>
<h4 data-number="1.2.1.1" id="agent-and-policy"><span
class="header-section-number">1.2.1.1</span> Agent and Policy</h4>
<ul>
<li><strong>Agent</strong>: The entity that acts in the
environment.</li>
<li><strong>Policy</strong>: The decision-making strategy of the
agent.</li>
</ul>
<h4 data-number="1.2.1.2" id="the-randomagent"><span
class="header-section-number">1.2.1.2</span> The RandomAgent</h4>
<p>We implement a RandomAgent class, which serves as our initial, naive
agent.</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># random_agent.py</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RandomAgent:</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, actions):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">        Initializes a RandomAgent object.</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">        - actions (list): A list of possible actions the agent can take.</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.actions <span class="op">=</span> actions</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_action(<span class="va">self</span>, state):</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Chooses the next action at random from the list of possible actions.</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="co">        - state: The current state of the agent (not used in this random policy).</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="co">        A randomly selected action from the agent&#39;s list of possible actions.</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random.choice(<span class="va">self</span>.actions)</span></code></pre></div>
<h4 data-number="1.2.1.3" id="agent-actions-in-gridworld"><span
class="header-section-number">1.2.1.3</span> Agent Actions in
Gridworld</h4>
<p>The agent in the Gridworld can perform four basic actions: moving up,
down, left, and right. These actions are represented as tuples, where
each tuple denotes the change in the agent’s position on the grid:</p>
<ol type="1">
<li><p><strong>Up</strong>: To move up, the agent decreases its
y-coordinate. This action is represented as <code>(0, -1)</code>,
meaning there is no change in the x-coordinate, and the y-coordinate
decreases by 1.</p></li>
<li><p><strong>Down</strong>: To move down, the agent increases its
y-coordinate. This action is represented as <code>(0, 1)</code>,
indicating no change in the x-coordinate and an increase of 1 in the
y-coordinate.</p></li>
<li><p><strong>Left</strong>: Moving left decreases the x-coordinate.
The action is represented as <code>(-1, 0)</code>, where the
x-coordinate decreases by 1 and there is no change in the
y-coordinate.</p></li>
<li><p><strong>Right</strong>: Moving right increases the x-coordinate.
This action is represented as <code>(1, 0)</code>, meaning the
x-coordinate increases by 1 with no change in the y-coordinate.</p></li>
</ol>
<h3 data-number="1.2.2" id="summary-and-next-steps"><span
class="header-section-number">1.2.2</span> Summary and Next Steps</h3>
<p>We introduced a simple agent to our Gridworld project. This sets the
foundation for more advanced learning algorithms to come.</p>
<h2 data-number="1.3" id="part-3-the-complete-code"><span
class="header-section-number">1.3</span> Part 3: The Complete Code:</h2>
<p>We combine the elements from Part 1 and Part 2 to run our Gridworld
simulation with the RandomAgent. The code for the agent’s interaction
with the Gridworld is in the gridworld_random_agent.py file.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># gridworld_random_agent.py</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gridworld <span class="im">import</span> Gridworld</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> random_agent <span class="im">import</span> RandomAgent</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the Gridworld environment</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>gridworld <span class="op">=</span> Gridworld(width<span class="op">=</span><span class="dv">5</span>, height<span class="op">=</span><span class="dv">5</span>, start<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>), goal<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>), obstacles<span class="op">=</span>[(<span class="dv">1</span>, <span class="dv">1</span>), (<span class="dv">2</span>, <span class="dv">2</span>), (<span class="dv">3</span>, <span class="dv">3</span>)])</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the actions and create the RandomAgent</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>actions <span class="op">=</span> [(<span class="dv">0</span>, <span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">0</span>), (<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>), (<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>)]  <span class="co"># Actions corresponding to [Up, Right, Down, Left]</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> RandomAgent(actions)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>total_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the agent in the environment</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>episodes <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(episodes):  <span class="co"># Run for a certain number of steps or until the goal is reached</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    current_state <span class="op">=</span> gridworld.agent_position</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    action <span class="op">=</span> agent.choose_action(current_state)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    new_state, reward, done <span class="op">=</span> gridworld.step(action)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    total_reward <span class="op">+=</span> reward</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> done:</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&quot;Goal reached!&quot;</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Total reward: </span><span class="sc">{</span>total_reward<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># if at the last step, and not done, print the total reward</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> _ <span class="op">==</span> episodes <span class="op">-</span> <span class="dv">1</span> <span class="kw">and</span> <span class="kw">not</span> done:</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&quot;Goal not reached!&quot;</span>)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Total reward: </span><span class="sc">{</span>total_reward<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<h3 data-number="1.3.1" id="why-these-actions"><span
class="header-section-number">1.3.1</span> Why These Actions?</h3>
<p>We selected standard grid movement actions for simplicity and to
establish a baseline for agent behavior.</p>
<h3 data-number="1.3.2" id="following-equations-and-logic"><span
class="header-section-number">1.3.2</span> Following Equations and
Logic</h3>
<p>At this stage, the agent’s decisions are random. We’ll later
introduce more sophisticated strategies based on RL algorithms and
mathematical foundations.</p>
<h3 data-number="1.3.3" id="summary-1"><span
class="header-section-number">1.3.3</span> Summary</h3>
<p>Chapter 1 sets the stage for an exploration into reinforcement
learning, providing the fundamental concepts and a practical project to
apply these ideas. As we progress, the agents will evolve from making
random moves to employing advanced strategies informed by their
interactions with the environment.</p>
<h1 data-number="2" id="chapter-2-basics-of-q-learning"><span
class="header-section-number">2</span> Chapter 2: Basics of
Q-Learning</h1>
<h3 data-number="2.0.1" id="introduction-to-q-learning"><span
class="header-section-number">2.0.1</span> Introduction to
Q-Learning</h3>
<p>Q-Learning is a foundational model-free reinforcement learning
algorithm that enables agents to learn optimal policies for
decision-making without knowing the dynamics of the environment they
operate in. By interacting with the environment, the agent learns to
associate actions with rewards and discovers the best strategy through
trial and error.</p>
<p>The term “Model-free” in the context of Q-Learning refers to a
specific approach within reinforcement learning, characterized by:</p>
<ol type="1">
<li><p>No Environment Dynamics Knowledge: In model-free Q-Learning, the
algorithm does not require prior knowledge of the environment’s
dynamics. This means it does not need to know how the environment will
respond to its actions, including state transitions and expected
outcomes.</p></li>
<li><p>Learning Through Experience: The agent learns by interacting with
the environment, taking actions, observing outcomes, and updating its
knowledge based on the results. This is often done through a Q-table,
mapping state-action pairs to expected rewards.</p></li>
<li><p>Flexibility and Generality: Model-free methods are applicable to
a wide range of environments, especially useful when the environment is
complex, unpredictable, or not fully understood.</p></li>
<li><p>Exploration vs. Exploitation: A key challenge in model-free
learning is balancing exploration (trying new actions for more
information) and exploitation (using known information to maximize
rewards), enabling the algorithm to learn effectively while performing
optimally.</p></li>
</ol>
<p>This approach allows agents to operate effectively in environments
where the dynamics are unknown or too complex to model.</p>
<h3 data-number="2.0.2" id="understanding-q-values"><span
class="header-section-number">2.0.2</span> Understanding Q-values</h3>
<p>At the core of Q-Learning are the Q-values, which represent the
expected cumulative reward of taking an action in a given state and
following the optimal policy thereafter. The Q-value function is denoted
by Q(s, a), where ‘s’ stands for state and ‘a’ for action.</p>
<h3 data-number="2.0.3"
id="the-q-learning-algorithm-process-and-components"><span
class="header-section-number">2.0.3</span> The Q-Learning Algorithm:
Process and Components</h3>
<p>The Q-Learning algorithm can be summarized in several key steps:</p>
<ol type="1">
<li><p>Initialize the Q-values table (Q-table) arbitrarily for all
state-action pairs. The Q-table is generally initialized with
zeros.</p></li>
<li><p>Observe the current state <code>s</code>.</p></li>
<li><p>Choose an action <code>a</code> for the state <code>s</code>
based on a policy derived from the Q-values (e.g., ε-greedy).</p>
<ul>
<li>A “policy” in this context is a strategy or rule that the agent
follows to decide which action to take in a given state. It guides the
action selection process based on the current Q-values, balancing
exploration (trying new actions) and exploitation (using known valuable
actions).</li>
</ul></li>
<li><p>Take the action <code>a</code>, and observe the outcome state
<code>s'</code>(Pronounced “S Prime”) and reward <code>r</code>.</p>
<ul>
<li>The outcome state <code>s'</code> refers to the new state the agent
finds itself in after taking action <code>a</code>. It represents the
next state in the environment as a result of the agent’s action.</li>
<li>The reward <code>r</code> is a feedback signal received from the
environment. It indicates how good or bad the action taken was, based on
the environment’s rules. Rewards guide the agent to learn which actions
are beneficial in achieving its goals.</li>
</ul></li>
<li><p>Update the Q-value for the state-action pair based on the
formula:</p>
<p>Q(s, a) = Q(s, a) + α * [r + γ * maxQ(s’, a’) - Q(s, a)]</p>
<p>The formula is a key part of reinforcement learning, specifically in
Q-learning. It’s used for updating the value of the Q-function, which
estimates the rewards for a given state-action pair. Below is the
breakdown of each component:</p>
<ol type="1">
<li><strong>Q(s, a)</strong>:</li>
</ol>
<ul>
<li>Represents the current estimate of the rewards that can be gained by
taking action ‘a’ in state ‘s’.</li>
</ul>
<ol start="2" type="1">
<li><strong>α (Alpha)</strong>:</li>
</ol>
<ul>
<li>The learning rate.</li>
<li>Determines to what extent the newly acquired information overrides
the old information.</li>
</ul>
<ol start="3" type="1">
<li><strong>r</strong>:</li>
</ol>
<ul>
<li>The reward received after taking action ‘a’ in state ‘s’.</li>
</ul>
<ol start="4" type="1">
<li><strong>γ (Gamma)</strong>:</li>
</ol>
<ul>
<li>The discount factor.</li>
<li>Determines the importance of future rewards.</li>
</ul>
<ol start="5" type="1">
<li><strong>max Q(s’, a’)</strong>:</li>
</ol>
<ul>
<li>Represents the maximum predicted reward that can be achieved in the
new state ‘s’ after taking action ‘a’.</li>
</ul>
<ol start="6" type="1">
<li><strong>Q(s, a) [Second Occurrence]</strong>:</li>
</ol>
<ul>
<li>The old value of the Q-function, which is being updated.</li>
</ul></li>
<li><p>Set the state <code>s</code> to the new state
<code>s'</code>.</p></li>
<li><p>If the end of the episode is not reached, go back to step
3.</p></li>
<li><p>Repeat these steps for many episodes to train the agent.</p></li>
</ol>
<p>The components of the Q-learning algorithm are:</p>
<ul>
<li><strong>Q-table</strong>: A lookup table where Q-values are stored
for each state-action pair.</li>
<li><strong>Policy</strong>: A strategy that the agent employs to
determine the next action based on the Q-table.</li>
<li><strong>Learning Rate (α)</strong>: Determines how much new
information overrides old information.</li>
<li><strong>Discount Factor (γ)</strong>: Measures the importance of
future rewards over immediate ones.</li>
<li><strong>Reward (r)</strong>: The signal received from the
environment to evaluate the last action.</li>
</ul>
<p>The formula in english: “The value of Q at state ‘s’ and action ‘a’
is updated to be equal to its old value plus the product of the learning
rate, alpha, and the difference between the immediate reward ‘r’ and the
discounted maximum future reward from the new state ‘s’ prime and any
action ‘a’ prime, minus the old Q value at state ‘s’ and action ‘a’. In
simpler terms, this formula adjusts the Q value for a given state-action
pair based on the immediate reward received, the best possible future
rewards, and how much we value future rewards compared to immediate
ones.”</p>
<p>This description conveys the intent of the formula, which is to
update the Q value based on new information about rewards and future
possibilities.</p>
<h3 data-number="2.0.4" id="convergence-of-q-learning"><span
class="header-section-number">2.0.4</span> Convergence of
Q-Learning</h3>
<p>In the context of Q-Learning, convergence refers to the point at
which the Q-values stop updating significantly and remain stable. This
means that the algorithm has learned the optimal policy, that is, the
best action to take in each state to maximize its future rewards.</p>
<p>When the Q-Learning algorithm converges, the agent’s knowledge about
which actions to take in the various states is as good as it can get for
a given environment. From this point, running more training episodes
will no longer change the agent’s behavior or improve its
performance.</p>
<p>It is important to note that convergence to the optimal Q-values
assumes that proper conditions have been met. This includes assumptions
like infinite visits to each state-action pair (ensuring sufficient
exploration), appropriate setting of parameters such as the learning
rate and discount factor, among other considerations.</p>
<hr />
<h3 data-number="2.0.5" id="project-q-learning-in-a-grid-world"><span
class="header-section-number">2.0.5</span> Project: Q-Learning in a Grid
World</h3>
<p>We’ll build a Q-Learning agent to navigate a Grid World environment.
This project will solidify the concepts presented in this chapter and
provide practical experience with the algorithm.</p>
<h4 data-number="2.0.5.1"
id="setting-up-a-simple-grid-world-environment"><span
class="header-section-number">2.0.5.1</span> Setting Up a Simple Grid
World Environment</h4>
<p>We’ll use the Gridworld environment from Chapter 1, which provides
the necessary methods to simulate an agent’s interaction with the
environment.</p>
<h4 data-number="2.0.5.2" id="implementing-q-learning-algorithm"><span
class="header-section-number">2.0.5.2</span> Implementing Q-Learning
Algorithm</h4>
<p>We will implement the Q-Learning algorithm in a Python script called
q_learning_agent.py. The script will interface with the Gridworld class
to train the agent.</p>
<h4 data-number="2.0.5.3"
id="defining-states-and-actions-in-the-grid-world"><span
class="header-section-number">2.0.5.3</span> Defining States and Actions
in the Grid World</h4>
<p>The Grid World environment will consist of discrete states and
actions. The agent can move in four directions: up, down, left, and
right.</p>
<h4 data-number="2.0.5.4" id="experimenting-with-q-value-updates"><span
class="header-section-number">2.0.5.4</span> Experimenting with Q-Value
Updates</h4>
<p>The agent will update its Q-values based on the rewards it receives
from the environment. The goal is for the agent to learn how to reach
the goal efficiently.</p>
<hr />
<p>Now, let’s begin our project with the q_learning_agent.py script.</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># q_learning_agent.py</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gridworld <span class="im">import</span> Gridworld</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up logging</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>logging.basicConfig(level<span class="op">=</span>logging.INFO, <span class="bu">format</span><span class="op">=</span><span class="st">&#39;</span><span class="sc">%(asctime)s</span><span class="st"> - </span><span class="sc">%(levelname)s</span><span class="st"> - </span><span class="sc">%(message)s</span><span class="st">&#39;</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>logger <span class="op">=</span> logging.getLogger()</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the Q-learning agent</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QLearningAgent:</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, alpha, gamma, epsilon, action_space, state_space_size):</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> alpha  <span class="co"># Learning rate</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> gamma  <span class="co"># Discount factor</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon <span class="op">=</span> epsilon  <span class="co"># Exploration rate</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action_space <span class="op">=</span> action_space  <span class="co"># Available actions</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q <span class="op">=</span> np.zeros((state_space_size, <span class="bu">len</span>(action_space)))  <span class="co"># Q-value table</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_action(<span class="va">self</span>, state):</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;Choose an action using an ε-greedy policy.&quot;&quot;&quot;</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> random.random() <span class="op">&lt;</span> <span class="va">self</span>.epsilon:</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Randomly choose an index corresponding to an action</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> random.randint(<span class="dv">0</span>, <span class="bu">len</span>(<span class="va">self</span>.action_space) <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Choose the index of the action with the highest Q-value</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> np.argmax(<span class="va">self</span>.Q[state])</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_Q(<span class="va">self</span>, state, action, reward, next_state):</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;Update the Q-value for the state-action pair using NumPy for efficient computation.&quot;&quot;&quot;</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        max_future_q <span class="op">=</span> np.<span class="bu">max</span>(<span class="va">self</span>.Q[next_state])</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>        current_q <span class="op">=</span> <span class="va">self</span>.Q[state, action]</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>        new_q <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.alpha) <span class="op">*</span> current_q <span class="op">+</span> <span class="va">self</span>.alpha <span class="op">*</span> (reward <span class="op">+</span> <span class="va">self</span>.gamma <span class="op">*</span> max_future_q)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q[state, action] <span class="op">=</span> new_q</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, env, num_episodes):</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;Train the agent over a specified number of episodes.&quot;&quot;&quot;</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>        action_space <span class="op">=</span> [(<span class="dv">0</span>, <span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">0</span>), (<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>), (<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>)]  <span class="co"># Actions: right, down, left, up</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>        total_rewards <span class="op">=</span> []</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>        steps_per_episode <span class="op">=</span> []</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(num_episodes):</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> env.reset()</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>            done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>            total_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>            steps <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>            <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>                action_index <span class="op">=</span> <span class="va">self</span>.choose_action(state)</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>                next_state, reward, done <span class="op">=</span> env.step(action_space[action_index])</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.update_Q(state, action_index, reward, next_state)</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>                state <span class="op">=</span> next_state</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>                total_reward <span class="op">+=</span> reward</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>                steps <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>            total_rewards.append(total_reward)</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>            steps_per_episode.append(steps)</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Log episode information</span></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>            logger.info(<span class="ss">f&#39;Episode </span><span class="sc">{</span>episode <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_episodes<span class="sc">}</span><span class="ss"> - &#39;</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>                        <span class="ss">f&#39;Total Reward: </span><span class="sc">{</span>total_reward<span class="sc">}</span><span class="ss">, &#39;</span></span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>                        <span class="ss">f&#39;Steps: </span><span class="sc">{</span>steps<span class="sc">}</span><span class="ss">, &#39;</span></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>                        <span class="ss">f&#39;Epsilon: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>epsilon<span class="sc">:.4f}</span><span class="ss">&#39;</span>)</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># After training, log summary statistics</span></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>        logger.info(<span class="ss">f&#39;Training complete. Average reward per episode: </span><span class="sc">{</span>np<span class="sc">.</span>mean(total_rewards)<span class="sc">:.2f}</span><span class="ss">&#39;</span>)</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>        logger.info(<span class="ss">f&#39;Average steps per episode: </span><span class="sc">{</span>np<span class="sc">.</span>mean(steps_per_episode)<span class="sc">:.2f}</span><span class="ss">&#39;</span>)</span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Main execution starts here</span></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&#39;__main__&#39;</span>:</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define the environment parameters</span></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>    env_params <span class="op">=</span> {</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;width&#39;</span>: <span class="dv">5</span>,</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;height&#39;</span>: <span class="dv">5</span>,</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;start&#39;</span>: (<span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;goal&#39;</span>: (<span class="dv">4</span>, <span class="dv">4</span>),</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;obstacles&#39;</span>: [(<span class="dv">1</span>, <span class="dv">1</span>), (<span class="dv">2</span>, <span class="dv">2</span>), (<span class="dv">3</span>, <span class="dv">3</span>)]</span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the Gridworld environment</span></span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a>    env <span class="op">=</span> Gridworld(<span class="op">**</span>env_params)</span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Map the actions to indices for the Q-table</span></span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>    action_space <span class="op">=</span> [(<span class="dv">0</span>, <span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">0</span>), (<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>), (<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>)]  <span class="co"># Actions: right, down, left, up</span></span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a>    action_indices <span class="op">=</span> {action: idx <span class="cf">for</span> idx, action <span class="kw">in</span> <span class="bu">enumerate</span>(action_space)}</span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the state space size</span></span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a>    state_space_size <span class="op">=</span> env.width <span class="op">*</span> env.height</span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the Q-learning agent with NumPy Q-table</span></span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a>    agent <span class="op">=</span> QLearningAgent(alpha<span class="op">=</span><span class="fl">0.1</span>, gamma<span class="op">=</span><span class="fl">0.9</span>, epsilon<span class="op">=</span><span class="fl">0.1</span>, action_space<span class="op">=</span>action_indices, state_space_size<span class="op">=</span>state_space_size)</span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Number of episodes to train the agent</span></span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>    num_episodes <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train the agent</span></span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a>    agent.train(env, num_episodes)</span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a>     <span class="co"># Save the trained Q-value table to a file</span></span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a>    np.savetxt(<span class="st">&#39;q_values.txt&#39;</span>, agent.Q)</span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="st">&quot;Training complete. Q-values saved to q_values.txt&quot;</span>)</span></code></pre></div>
<p>This script sets up the Q-Learning agent, defines its methods, and
runs the training process. The agent is tested in the
<code>Gridworld</code> environment and its Q-values are saved to a file
after training.</p>
<p>Continuing with the remaining sections of the chapter and further
description of the code:</p>
<h3 data-number="2.0.6" id="running-the-training-process"><span
class="header-section-number">2.0.6</span> Running the Training
Process</h3>
<p>With the <code>q_learning_agent.py</code> script defined, we can now
discuss the execution of the training process. The main objective is to
run the agent through a series of episodes in the <code>Gridworld</code>
environment to learn the optimal Q-values.</p>
<p>Training begins by initializing the environment and the agent. At the
start of each episode, the environment is reset to its initial state,
and the agent initializes the Q-values for that state if they haven’t
been seen before. The agent then repeatedly chooses actions, observes
the resulting new states and rewards, and updates the Q-values until the
episode ends (either the goal is reached or a terminal condition is
met).</p>
<p>The <code>train</code> method encapsulates this process, logging the
completion of each episode to help us monitor the agent’s progress.
After training for the specified number of episodes, the learned
Q-values are saved to a file. This data can be used for analysis or to
initialize the agent at a later time for further learning or policy
execution.</p>
<h3 data-number="2.0.7" id="analyzing-the-results"><span
class="header-section-number">2.0.7</span> Analyzing the Results</h3>
<p>After training, it’s beneficial to analyze the results. You can do
this by looking at the Q-values to understand what the agent has
learned. Checking the convergence of Q-values can indicate whether the
agent has learned a stable policy.</p>
<p>Another approach is to visualize the agent’s behavior in the
<code>Gridworld</code> by running it with the learned policy and
observing its actions. This can be done by modifying the
<code>train</code> method to include a rendering of the environment
after each action or by creating a separate method for policy execution
where the agent only selects the best-known action without further
exploration.</p>
<h3 data-number="2.0.8" id="considerations-for-improving-learning"><span
class="header-section-number">2.0.8</span> Considerations for Improving
Learning</h3>
<p>Several factors can influence the effectiveness of Q-Learning: -
<strong>Learning Rate (α):</strong> Determines how much new information
overrides old information. A smaller α makes the learning updates more
conservative. - <strong>Discount Factor (γ):</strong> Reflects the
importance of future rewards. A higher γ values future rewards more,
while a lower γ results in a more myopic policy. - <strong>Exploration
Rate (ε):</strong> Balances exploration and exploitation. Initially, a
higher ε encourages the agent to explore the environment. Over time,
gradually reducing ε (known as epsilon decay) can lead to better
exploitation of the learned values. - <strong>Initial Q-values:</strong>
Setting initial Q-values to optimistic estimates can encourage
exploration. This is known as optimistic initialization.</p>
<h3 data-number="2.0.9" id="conclusion-and-next-steps"><span
class="header-section-number">2.0.9</span> Conclusion and Next
Steps</h3>
<p>By implementing the Q-Learning algorithm in a practical example such
as the <code>Gridworld</code>, we have gained a deeper understanding of
how agents learn from their environment to make optimal decisions. With
the foundation built in this chapter, we can extend our knowledge to
more complex reinforcement learning algorithms and problems.</p>
<p>As a next step, consider experimenting with different parameters (α,
γ, and ε), introducing variability in rewards, or increasing the
complexity of the <code>Gridworld</code>. You can also implement
enhancements to the Q-Learning algorithm, such as using experience
replay or function approximation techniques for larger state spaces.</p>
<p>With the project and the chapter content completed, you now have a
solid understanding of Q-Learning and the know-how to implement it from
scratch. The next chapters will build on this knowledge and introduce
you to advanced topics in reinforcement learning.</p>
<hr />
<p>This concludes the full chapter and project on the basics of
Q-Learning. The chapter has introduced the key concepts, algorithmic
components, and practical application in a simple Grid World
environment. The project has provided a hands-on experience with coding
a Q-Learning agent and running it to learn a policy for navigating the
Grid World.</p>
<h1 data-number="3"
id="chapter-3-fundamentals-of-deep-learning-for-reinforcement-learning"><span
class="header-section-number">3</span> Chapter 3: Fundamentals of Deep
Learning for Reinforcement Learning</h1>
<h2 data-number="3.1" id="overview-1"><span
class="header-section-number">3.1</span> Overview</h2>
<p>This chapter explores the intersection of deep learning and
reinforcement learning, forming the emerging field of deep reinforcement
learning (DRL). Our goal is to clearly explain the fundamental concepts
of deep learning and demonstrate how combining it with RL enables
effective solutions for complex decision-making problems.</p>
<h2 data-number="3.2" id="objectives-1"><span
class="header-section-number">3.2</span> Objectives</h2>
<ul>
<li>Understand the fundamental concepts of deep learning, with a focus
on neural networks.</li>
<li>Learn how deep learning can be integrated with RL algorithms to
improve their performance.</li>
<li>Prepare to implement a deep Q-network (DQN) in the Gridworld
project.</li>
</ul>
<h2 data-number="3.3" id="introduction-to-neural-networks"><span
class="header-section-number">3.3</span> Introduction to Neural
Networks</h2>
<p>Neural networks are designed to mimic the workings of the human brain
and are made up of interconnected units called neurons. These networks
have the capacity to learn from data, enabling them to perform tasks
such as predictions or decision-making without the need for explicit
programming.</p>
<h3 data-number="3.3.1" id="key-concepts-in-neural-networks"><span
class="header-section-number">3.3.1</span> Key Concepts in Neural
Networks</h3>
<ul>
<li><strong>Neurons</strong>: The building blocks of neural networks
that take input, process it using a weighted sum followed by an
activation function, and produce an output.</li>
<li><strong>Layers</strong>: A neural network comprises different layers
of neurons, which include the input layer, one or more hidden layers,
and the output layer.</li>
<li><strong>Weights and Biases</strong>: Parameters of the model that
adjust during training to make the neural network’s predictions as
accurate as possible.</li>
</ul>
<h2 data-number="3.4" id="how-neural-networks-learn"><span
class="header-section-number">3.4</span> How Neural Networks Learn</h2>
<p>The process of learning in neural networks involves adjusting their
weights and biases. The goal is to reduce the discrepancy between the
model’s predictions and the actual target values.</p>
<ul>
<li><strong>Forward Propagation</strong>: This is the process where
input data is passed through the network to generate output
predictions.</li>
<li><strong>Loss Functions</strong>: These are measures used to assess
the deviation between the network’s predictions and the true
outcomes.</li>
<li><strong>Backpropagation</strong>: A key algorithm in neural network
training, backpropagation calculates the gradient of the loss function
with respect to each weight and bias, using these gradients to update
the parameters via gradient descent.</li>
</ul>
<h2 data-number="3.5" id="deep-learning-in-reinforcement-learning"><span
class="header-section-number">3.5</span> Deep Learning in Reinforcement
Learning</h2>
<p>Deep learning equips RL with powerful function approximation
capabilities, essential for dealing with high-dimensional state or
action spaces.</p>
<h3 data-number="3.5.1" id="deep-reinforcement-learning"><span
class="header-section-number">3.5.1</span> Deep Reinforcement
Learning</h3>
<ul>
<li><strong>Function Approximation</strong>: The application of deep
neural networks to estimate value functions or policy functions.</li>
<li><strong>Advantages of Deep RL</strong>: These include the ability to
process complex inputs, generalize across different states, and discern
underlying patterns in the data.</li>
</ul>
<h2 data-number="3.6" id="enhancing-gridworld-with-deep-learning"><span
class="header-section-number">3.6</span> Enhancing Gridworld with Deep
Learning</h2>
<p>To integrate deep learning into our Gridworld example, we will employ
a deep Q-network (DQN). A DQN utilizes a neural network to estimate the
Q-value function, a central concept in RL that predicts the quality of
actions taken in various states.</p>
<ul>
<li><strong>Project Update</strong>: We will implement a DQN to
substitute the Q-table with a neural network that predicts Q-values for
Gridworld.</li>
<li><strong>Implementation Steps</strong>: We outline the necessary
modifications to the existing Gridworld project to incorporate a
DQN.</li>
</ul>
<h2 data-number="3.7" id="summary-2"><span
class="header-section-number">3.7</span> Summary</h2>
<p>We recap the deep learning concepts introduced in this chapter and
discuss their relevance to RL. This provides the foundation for a
hands-on implementation of a DQN in the following sections.</p>
<hr />
<h2 data-number="3.8"
id="implementing-a-deep-q-network-dqn-with-pytorch"><span
class="header-section-number">3.8</span> Implementing a Deep Q-Network
(DQN) with PyTorch</h2>
<p>We use PyTorch to construct our DQN. The following parts detail each
section of the code.</p>
<p>Create a new file called <code>deep_q_learning_agent.py</code> and
import the necessary libraries:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gridworld <span class="im">import</span> Gridworld  <span class="co"># This imports the Gridworld environment</span></span></code></pre></div>
<h3 data-number="3.8.1" id="defining-the-network-architecture"><span
class="header-section-number">3.8.1</span> Defining the Network
Architecture</h3>
<p>We start by defining our neural network:</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the neural network architecture for Deep Q-Learning</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleMLP(nn.Module):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Constructor for the neural network</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, output_size, hidden_size<span class="op">=</span><span class="dv">64</span>):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SimpleMLP, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First fully connected layer from input_size to hidden_size</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, hidden_size)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Second fully connected layer, hidden to hidden</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(hidden_size, hidden_size)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Third fully connected layer from hidden_size to output_size</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(hidden_size, output_size)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass definition for the neural network</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply ReLU activation function after first layer</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply ReLU activation function after second layer</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output layer, no activation function</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.fc3(x)</span></code></pre></div>
<p>The <code>SimpleMLP</code> class represents a multilayer perceptron
with layers <code>fc1</code>, <code>fc2</code>, and
<code>fc3</code>.</p>
<h3 data-number="3.8.2" id="dqn-agent-class"><span
class="header-section-number">3.8.2</span> DQN Agent Class</h3>
<p>Next, we have the <code>DQNAgent</code> class, controlling action
selection and learning from interactions with the environment:</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DQNAgent:</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Constructor for the DQN agent</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, output_size, hidden_size<span class="op">=</span><span class="dv">64</span>):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The neural network model</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> SimpleMLP(input_size, output_size, hidden_size)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Epsilon for the epsilon-greedy policy (initially set to 1 for full exploration)</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Minimum value that epsilon can decay to over time</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon_min <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Rate at which epsilon is decayed over time</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon_decay <span class="op">=</span> <span class="fl">0.995</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># List to hold past experiences for replay</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.memory <span class="op">=</span> []</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Method to decide an action based on the current state</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> act(<span class="va">self</span>, state):</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check if we should take a random action (exploration)</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.random.rand() <span class="op">&lt;=</span> <span class="va">self</span>.epsilon:</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Return a random action within the action space size</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> random.randrange(output_size)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If not exploring, process the state through the DQN to get the action values</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First, ensure state is a PyTorch tensor</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(state, torch.Tensor):</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> torch.from_numpy(state).<span class="bu">float</span>().unsqueeze(<span class="dv">0</span>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass through the network to get action values</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>        action_values <span class="op">=</span> <span class="va">self</span>.model(state)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return the action with the highest value</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.argmax(action_values.detach().numpy())</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Method to store experiences in replay memory</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> remember(<span class="va">self</span>, state, action, reward, next_state, done):</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Append the experience as a tuple to the memory list</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.memory.append((state, action, reward, next_state, done))</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Method to decay epsilon over time for less exploration</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_epsilon(<span class="va">self</span>):</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.epsilon <span class="op">&gt;</span> <span class="va">self</span>.epsilon_min:</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.epsilon <span class="op">*=</span> <span class="va">self</span>.epsilon_decay</span></code></pre></div>
<p>Here, the agent is defined with methods like <code>act</code> for
decision-making and constructors for agent properties.</p>
<h3 data-number="3.8.3" id="training-the-model"><span
class="header-section-number">3.8.3</span> Training the Model</h3>
<p>The <code>train_model</code> function updates the network’s
parameters using experience replay:</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model based on a batch of experience</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(agent, optimizer, batch_size, gamma):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check that there are enough experiences in memory to sample a batch</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(agent.memory) <span class="op">&lt;</span> batch_size:</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample a minibatch of experiences from memory</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    minibatch <span class="op">=</span> random.sample(agent.memory, batch_size)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Unpack the experiences</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    states, actions, rewards, next_states, dones <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>minibatch)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert experience components to PyTorch tensors</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    states <span class="op">=</span> torch.from_numpy(np.vstack(states)).<span class="bu">float</span>()</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    actions <span class="op">=</span> torch.from_numpy(np.vstack(actions)).<span class="bu">long</span>()</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    rewards <span class="op">=</span> torch.from_numpy(np.vstack(rewards)).<span class="bu">float</span>()</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    next_states <span class="op">=</span> torch.from_numpy(np.vstack(next_states)).<span class="bu">float</span>()</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    dones <span class="op">=</span> torch.from_numpy(np.vstack(dones).astype(np.uint8)).<span class="bu">float</span>()</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the expected Q values from the neural network</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    Q_expected <span class="op">=</span> agent.model(states).gather(<span class="dv">1</span>, actions)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the Q value for the next states and get the max Q value for each next state</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    Q_targets_next <span class="op">=</span> agent.model(next_states).detach().<span class="bu">max</span>(<span class="dv">1</span>)[<span class="dv">0</span>].unsqueeze(<span class="dv">1</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the target Q values for the current states using the Bellman equation</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    Q_targets <span class="op">=</span> rewards <span class="op">+</span> (gamma <span class="op">*</span> Q_targets_next <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> dones))</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the loss between the expected Q values and the target Q values</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.mse_loss(Q_expected, Q_targets)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backpropagate the loss, update the network weights</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span></code></pre></div>
<p>This function calculates the loss between the predicted and target
Q-values and performs backpropagation to update the network weights.</p>
<h3 data-number="3.8.4" id="logging-agents-performance"><span
class="header-section-number">3.8.4</span> Logging Agent’s
Performance</h3>
<p>Finally, a simple logging function keeps track of the agent’s
learning progress:</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Log the performance metrics</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_performance(episode, total_reward, steps, epsilon):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print out the episode number, total reward, number of steps and the epsilon value</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Episode: </span><span class="sc">{</span>episode<span class="sc">}</span><span class="ss">, Total Reward: </span><span class="sc">{</span>total_reward<span class="sc">}</span><span class="ss">, Steps: </span><span class="sc">{</span>steps<span class="sc">}</span><span class="ss">, Epsilon: </span><span class="sc">{</span>epsilon<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<p>This utility function prints information about the episode number,
total rewards, steps, and the epsilon value.</p>
<h3 data-number="3.8.5" id="example-code-execution"><span
class="header-section-number">3.8.5</span> Example Code Execution</h3>
<p>The main function demonstrates initializing the environment, agent,
training, and logging:</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the Gridworld environment</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    gridworld <span class="op">=</span> Gridworld(width<span class="op">=</span><span class="dv">5</span>, height<span class="op">=</span><span class="dv">5</span>, start<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>), goal<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>), obstacles<span class="op">=</span>[(<span class="dv">1</span>, <span class="dv">1</span>), (<span class="dv">2</span>, <span class="dv">2</span>), (<span class="dv">3</span>, <span class="dv">3</span>)])</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define action mapping that maps action indices to movements</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    action_mapping <span class="op">=</span> [(<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">0</span>), (<span class="dv">0</span>, <span class="dv">1</span>), (<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>)]  <span class="co"># Up, Right, Down, Left</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the input size based on the environment&#39;s state space</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    input_size <span class="op">=</span> gridworld.width <span class="op">*</span> gridworld.height</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The output size is the number of possible actions</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    output_size <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the DQN agent</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    agent <span class="op">=</span> DQNAgent(input_size, output_size)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define an optimizer for the neural network (Adam optimizer with a learning rate of 0.001)</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(agent.model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Batch size for experience replay</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Discount factor for future rewards</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    gamma <span class="op">=</span> <span class="fl">0.99</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Function to convert a state to a tensor for neural network input</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> state_to_tensor(state, grid_size<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a one-hot encoded vector for the state</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>        state_vector <span class="op">=</span> torch.zeros(grid_size <span class="op">*</span> grid_size, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        state_vector[state] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> state_vector</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Main training loop</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reset the environment at the start of each episode</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> gridworld.reset()</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize total reward and steps counter</span></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>        total_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>        done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>        steps <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop for each step in the episode</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Convert state to tensor format</span></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>            state_vector <span class="op">=</span> state_to_tensor(state)</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Select an action using the DQN agent&#39;s policy</span></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> agent.act(state_vector)</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Take the action in the environment and observe the next state and reward</span></span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>            next_state, reward, done <span class="op">=</span> gridworld.step(action_mapping[action])</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Convert the next state to tensor format</span></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>            next_state_vector <span class="op">=</span> state_to_tensor(next_state)</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Remember the experience</span></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>            agent.remember(state_vector, action, reward, next_state_vector, done)</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Move to the next state</span></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> next_state</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update the total reward</span></span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>            total_reward <span class="op">+=</span> reward</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Increment the step counter</span></span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>            steps <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Train the model with experiences from memory</span></span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>            train_model(agent, optimizer, batch_size, gamma)</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># After the episode, decay epsilon for less exploration in future episodes</span></span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>        agent.update_epsilon()</span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Log the performance metrics for the episode</span></span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a>        log_performance(episode, total_reward, steps, agent.epsilon)</span></code></pre></div>
<p>Within each episode, the agent selects actions, updates its Q-values,
and logs its performance.</p>
<hr />
<p>In this python application, we have outlined the creation of a simple
feedforward neural network model using PyTorch for approximating
Q-values in the Gridworld environment. The code includes a DQN agent
that selects actions using an epsilon-greedy policy and stores
experiences in a replay buffer. The <code>train_model</code> function
updates the neural network’s weights using sampled experiences to reduce
the loss between predicted and target Q-values.</p>
<p>Please note that this example is a simplified version of DQN. For
practical and more complex scenarios, additional mechanisms like
experience replay buffers with more sophisticated sampling techniques
and separate target networks to stabilize the Q-value predictions are
recommended.</p>
</body>
</html>
