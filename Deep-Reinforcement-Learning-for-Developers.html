<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Abdul Hagi" />
  <title>Deep Reinforcement Learning for Developers</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="css/style.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Deep Reinforcement Learning for Developers</h1>
<p class="author">Abdul Hagi</p>
</header>
<h1 id="chapter-1-introduction-to-reinforcement-learning">Chapter 1:
Introduction to Reinforcement Learning</h1>
<h2 id="part-1-understanding-the-fundamentals">Part 1: Understanding the
Fundamentals</h2>
<h3 id="overview">Overview</h3>
<p>Reinforcement learning (RL) is a paradigm in machine learning that
provides a framework for agents to learn how to behave in an environment
by performing actions and seeing the results. This chapter introduces
RL, explores its key concepts, and applies them in a simple Gridworld
environment.</p>
<h3 id="objectives">Objectives</h3>
<ul>
<li>Understand the core principles of RL.</li>
<li>Identify RL applications in different industries.</li>
<li>Establish a foundational project in RL.</li>
</ul>
<h3 id="what-is-reinforcement-learning">What is Reinforcement
Learning?</h3>
<p>RL involves an agent that learns to make decisions by taking actions
in an environment to maximize some notion of cumulative reward. It’s
characterized by trial and error, feedback loops, and adaptability to
changing situations.</p>
<h4 id="key-components-of-rl">Key Components of RL</h4>
<ul>
<li><strong>Agent</strong>: The learner or decision-maker.</li>
<li><strong>Environment</strong>: Where the agent takes actions.</li>
<li><strong>Action</strong>: A set of operations that the agent can
perform.</li>
<li><strong>State</strong>: The current situation of the
environment.</li>
<li><strong>Reward</strong>: Feedback from the environment to the
agent.</li>
</ul>
<h3 id="why-reinforcement-learning">Why Reinforcement Learning?</h3>
<p>Reinforcement learning is unique in its approach to problem-solving,
allowing agents to learn from their own experiences rather than being
told the correct actions. This is particularly useful in complex,
unpredictable environments or when the desired behavior is difficult to
express with explicit rules.</p>
<h3 id="applications-of-rl">Applications of RL</h3>
<ul>
<li><strong>Game playing</strong>: Achieving superhuman performance in
complex games.</li>
<li><strong>Robotics</strong>: Teaching robots to perform tasks
autonomously.</li>
<li><strong>Autonomous vehicles</strong>: Driving safely and efficiently
in dynamic environments.</li>
</ul>
<h3 id="project-overview-the-gridworld">Project Overview: The
Gridworld</h3>
<p>The Gridworld is an introductory RL project where an agent must
navigate through a grid to reach a goal while avoiding obstacles.</p>
<h4 id="project-goal">Project Goal</h4>
<p>Create an agent that finds the shortest path to a goal within a grid,
considering obstacles.</p>
<h4 id="key-learning">Key Learning</h4>
<p>This project will teach you about the interaction between an agent
and its environment, the role of rewards, and how to implement these
concepts in code.</p>
<h3 id="coding-the-gridworld-environment">Coding the Gridworld
Environment</h3>
<p>We construct a <code>Gridworld</code> class in Python to simulate the
environment for our agent.</p>
<ul>
<li>File: <code>gridworld.py</code></li>
</ul>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># gridworld.py</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Gridworld:</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, width, height, start, goal, obstacles):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">        Initializes a Gridworld object.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">        - width (int): The width of the grid.</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">        - height (int): The height of the grid.</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">        - start (tuple): The starting position of the agent as a tuple (x, y).</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">        - goal (tuple): The goal position of the agent as a tuple (x, y).</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">        - obstacles (list): A list of obstacle positions as tuples [(x1, y1), (x2, y2), ...].</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.width <span class="op">=</span> width</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.height <span class="op">=</span> height</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.start <span class="op">=</span> start</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.goal <span class="op">=</span> goal</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.obstacles <span class="op">=</span> obstacles</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reset()</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, action):</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co">        Takes an action in the environment and updates the agent&#39;s position.</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co">        - action (tuple): The action to be taken.</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co">        - new_state: The new state after taking the action.</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co">        - reward: The reward received after taking the action.</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co">        - done: A boolean indicating if the goal has been reached.</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the new position after taking the action</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        new_x <span class="op">=</span> <span class="va">self</span>.agent_position[<span class="dv">0</span>] <span class="op">+</span> action[<span class="dv">0</span>]</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        new_y <span class="op">=</span> <span class="va">self</span>.agent_position[<span class="dv">1</span>] <span class="op">+</span> action[<span class="dv">1</span>]</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check if the new position is within the grid bounds and not an obstacle</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (<span class="dv">0</span> <span class="op">&lt;=</span> new_x <span class="op">&lt;</span> <span class="va">self</span>.width) <span class="kw">and</span> (<span class="dv">0</span> <span class="op">&lt;=</span> new_y <span class="op">&lt;</span> <span class="va">self</span>.height) <span class="kw">and</span> <span class="kw">not</span> (new_x, new_y) <span class="kw">in</span> <span class="va">self</span>.obstacles:</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update the agent&#39;s position</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.agent_position <span class="op">=</span> (new_x, new_y)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check if the new position is the goal</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        done <span class="op">=</span> <span class="va">self</span>.agent_position <span class="op">==</span> <span class="va">self</span>.goal</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define the reward for reaching the goal or taking a step</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> <span class="dv">0</span> <span class="cf">if</span> done <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.get_state(), reward, done</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>):</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="co">        Resets the grid and agent position to the initial state.</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.agent_position <span class="op">=</span> <span class="va">self</span>.start</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grid <span class="op">=</span> np.zeros((<span class="va">self</span>.height, <span class="va">self</span>.width))</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> obstacle <span class="kw">in</span> <span class="va">self</span>.obstacles:</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grid[obstacle] <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grid[<span class="va">self</span>.goal] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.get_state()  <span class="co"># Return the initial state</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> render(<span class="va">self</span>):</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="co">        Renders the current state of the gridworld.</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> y <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.height):</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.width):</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> (x, y) <span class="op">==</span> <span class="va">self</span>.agent_position:</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="st">&#39;A&#39;</span>, end<span class="op">=</span><span class="st">&#39; &#39;</span>)  <span class="co"># Agent&#39;s position</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> (x, y) <span class="op">==</span> <span class="va">self</span>.goal:</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="st">&#39;G&#39;</span>, end<span class="op">=</span><span class="st">&#39; &#39;</span>)  <span class="co"># Goal position</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> (x, y) <span class="kw">in</span> <span class="va">self</span>.obstacles:</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="st">&#39;#&#39;</span>, end<span class="op">=</span><span class="st">&#39; &#39;</span>)  <span class="co"># Obstacle</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="st">&#39;.&#39;</span>, end<span class="op">=</span><span class="st">&#39; &#39;</span>)  <span class="co"># Empty cell</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>()  <span class="co"># Newline at the end of each row</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>()  <span class="co"># Extra newline for better separation between steps</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>gridworld <span class="op">=</span> Gridworld(width<span class="op">=</span><span class="dv">5</span>, height<span class="op">=</span><span class="dv">5</span>, start<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>), goal<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>), obstacles<span class="op">=</span>[(<span class="dv">1</span>, <span class="dv">1</span>), (<span class="dv">2</span>, <span class="dv">2</span>), (<span class="dv">3</span>, <span class="dv">3</span>)])</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>gridworld.render()</span></code></pre></div>
<h3 id="summary">Summary</h3>
<p>This section established the basics of RL and introduced a simple
Gridworld environment for future exploration.</p>
<h2 id="part-2-building-the-agent">Part 2: Building the Agent</h2>
<h3 id="implementing-the-agent">Implementing the Agent</h3>
<p>We now add an agent to interact with the Gridworld, beginning with a
basic agent that makes random moves.</p>
<h4 id="agent-and-policy">Agent and Policy</h4>
<ul>
<li><strong>Agent</strong>: The entity that acts in the
environment.</li>
<li><strong>Policy</strong>: The decision-making strategy of the
agent.</li>
</ul>
<h4 id="the-randomagent">The RandomAgent</h4>
<p>We implement a <code>RandomAgent</code> class, which serves as our
initial, naive agent.</p>
<ul>
<li>File: <code>gridworld_agent.py</code></li>
</ul>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># gridworld_agent.py</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RandomAgent:</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, actions):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">        Initializes a RandomAgent object.</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">        - actions (list): A list of possible actions the agent can take.</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.actions <span class="op">=</span> actions</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_action(<span class="va">self</span>, state):</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Chooses the next action at random from the list of possible actions.</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co">        - state: The current state of the agent (not used in this random policy).</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co">        A randomly selected action from the agent&#39;s list of possible actions.</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random.choice(<span class="va">self</span>.actions)</span></code></pre></div>
<h4 id="agent-actions-in-gridworld">Agent Actions in Gridworld</h4>
<p>The agent can move in four directions: up, down, left, and right.</p>
<h4 id="interactions-and-outcomes">Interactions and Outcomes</h4>
<p>We simulate the agent’s interactions with the Gridworld, using a
reward system to guide its learning.</p>
<h4 id="running-the-agent">Running the Agent</h4>
<p>We execute the agent within the Gridworld, observing its behavior
over time.</p>
<h3 id="summary-and-next-steps">Summary and Next Steps</h3>
<p>We introduced a simple agent to our Gridworld project. This sets the
foundation for more advanced learning algorithms to come.</p>
<hr />
<h1 id="full-chapter-1-the-complete-code">Full Chapter 1: The Complete
Code</h1>
<p>We combine the elements from Part 1 and Part 2 to run our Gridworld
simulation with the RandomAgent. The code for the agent’s interaction
with the Gridworld is in the <code>gridworld_random_agent.py</code>
file.</p>
<ul>
<li>File: <code>gridworld_random_agent.py</code></li>
</ul>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># gridworld_random_agent.py</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gridworld <span class="im">import</span> Gridworld</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gridworld_agent <span class="im">import</span> RandomAgent</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the Gridworld environment</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>gridworld <span class="op">=</span> Gridworld(width<span class="op">=</span><span class="dv">5</span>, height<span class="op">=</span><span class="dv">5</span>, start<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>), goal<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>), obstacles<span class="op">=</span>[(<span class="dv">1</span>, <span class="dv">1</span>), (<span class="dv">2</span>, <span class="dv">2</span>), (<span class="dv">3</span>, <span class="dv">3</span>)])</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the actions and create the RandomAgent</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>actions <span class="op">=</span> [(<span class="dv">0</span>, <span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">0</span>), (<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>), (<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>)]  <span class="co"># Actions corresponding to [Up, Right, Down, Left]</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> RandomAgent(actions)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>total_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the agent in the environment</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):  <span class="co"># Run for a certain number of steps or until the goal is reached</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    current_state <span class="op">=</span> gridworld.agent_position</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    action <span class="op">=</span> agent.choose_action(current_state)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    new_state, reward, done <span class="op">=</span> gridworld.step(action)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    total_reward <span class="op">+=</span> reward</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Action taken: </span><span class="sc">{</span>action<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Reward received: </span><span class="sc">{</span>reward<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Total reward: </span><span class="sc">{</span>total_reward<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    gridworld.render()</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> done:</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&quot;Goal reached!&quot;</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code></pre></div>
<h3 id="why-these-actions">Why These Actions?</h3>
<p>We selected standard grid movement actions for simplicity and to
establish a baseline for agent behavior.</p>
<h3 id="following-equations-and-logic">Following Equations and
Logic</h3>
<p>At this stage, the agent’s decisions are random. We’ll later
introduce more sophisticated strategies based on RL algorithms and
mathematical foundations.</p>
<h3 id="summary-1">Summary</h3>
<p>Chapter 1 sets the stage for an exploration into reinforcement
learning, providing the fundamental concepts and a practical project to
apply these ideas. As we progress, the agents will evolve from making
random moves to employing advanced strategies informed by their
interactions with the environment.</p>
<h1 id="chapter-2-introduction-to-q-learning">Chapter 2: Introduction to
Q-Learning</h1>
<h2 id="part-1-theoretical-foundations">Part 1: Theoretical
Foundations</h2>
<h3 id="overview-1">Overview</h3>
<p>This chapter explores the concept of Q-Learning, a model-free
reinforcement learning algorithm that enables agents to learn optimal
action-value functions and derive an optimal policy.</p>
<h3 id="objectives-1">Objectives</h3>
<ul>
<li>Introduce the Q-Learning algorithm and its role in reinforcement
learning.</li>
<li>Understand the components and operation of the Q-Learning
algorithm.</li>
<li>Prepare for the practical application of Q-Learning in the Gridworld
project.</li>
</ul>
<h3 id="what-is-q-learning">What is Q-Learning?</h3>
<p>Q-Learning is a method that allows agents to learn the value of an
action in a particular state, guiding them to make optimal decisions
through trial and error.</p>
<h4 id="key-concepts-of-q-learning">Key Concepts of Q-Learning</h4>
<ul>
<li><strong>Q-value (Q-function)</strong>: An estimation of the total
expected rewards an agent can get, given a state and action.</li>
<li><strong>Q-table</strong>: A table where Q-values for each
state-action pair are stored.</li>
</ul>
<h3 id="why-q-learning">Why Q-Learning?</h3>
<p>Q-Learning is advantageous as it can handle environments with
stochastic transitions and rewards without requiring a model of the
environment.</p>
<h3 id="the-q-learning-algorithm">The Q-Learning Algorithm</h3>
<p>The Q-Learning algorithm is a reinforcement learning technique used
to solve Markov Decision Processes (MDPs). It learns an optimal policy
by iteratively updating the Q-values of state-action pairs based on the
rewards received.</p>
<p>Algorithm Steps: Initialize the Q-table with zeros for all
state-action pairs.</p>
<p>Set the learning rate (alpha), discount factor (gamma), and
exploration rate (epsilon).</p>
<p>Repeat the following steps until convergence or a maximum number of
episodes:</p>
<ol type="a">
<li><p>Observe the current state (s).</p></li>
<li><p>Choose an action (a) based on the current state using an
exploration-exploitation strategy (e.g., epsilon-greedy).</p></li>
<li><p>Perform the chosen action and observe the next state (s’) and the
reward (r).</p></li>
<li><p>Update the Q-value of the current state-action pair using the
Q-Learning update equation:</p></li>
</ol>
<p>Q(s, a) = (1 - alpha) * Q(s, a) + alpha * (r + gamma * max(Q(s’,
a’))) e. Update the current state (s) to the next state (s’).</p>
<p>Return the learned Q-table, which represents the optimal action-value
function.</p>
<p>Parameters: alpha (learning rate): Determines the weight given to the
new information when updating the Q-values. A higher value gives more
weight to the new information.</p>
<p>gamma (discount factor): Controls the importance of future rewards. A
value closer to 1 considers long-term rewards, while a value closer to 0
focuses on immediate rewards.</p>
<p>epsilon (exploration rate): Determines the balance between
exploration and exploitation. A higher value encourages more
exploration, while a lower value favors exploitation of the learned
Q-values.</p>
<p>The Q-Learning algorithm is a powerful technique for solving
reinforcement learning problems. It allows an agent to learn an optimal
policy by iteratively updating the Q-values based on the rewards
received. By balancing exploration and exploitation, the agent can find
the best actions to take in different states to maximize its cumulative
reward.</p>
<h3 id="coding-q-learning-components">Coding Q-Learning Components</h3>
<p>To implement Q-Learning in our Gridworld project, we will create the
Q-table and implement the update rule as part of the agent’s learning
algorithm.</p>
<h3 id="summary-2">Summary</h3>
<p>In this section, we’ve introduced the theoretical underpinnings of
Q-Learning, setting the stage for a practical implementation in the
Gridworld environment.</p>
<h2 id="part-2-practical-implementation">Part 2: Practical
Implementation</h2>
<h3 id="implementing-q-learning-in-gridworld">Implementing Q-Learning in
Gridworld</h3>
<p>To extend our Gridworld project with the Q-Learning algorithm, we
will add functionality to our agent to learn from the environment using
a Q-table.</p>
<h4 id="q-table-initialization">Q-table Initialization</h4>
<p>We initialize a Q-table that maps each state-action pair to its
Q-value, which will be updated as the agent learns.</p>
<h4 id="q-learning-update">Q-Learning Update</h4>
<p>After taking an action, we update the Q-table entries using the
Q-Learning formula, which incorporates the reward received and the
estimated value of the next state.</p>
<h4 id="exploration-vs.-exploitation">Exploration vs. Exploitation</h4>
<p>We’ll implement an <span class="math inline"><em>ϵ</em></span>-greedy
strategy, enabling the agent to explore the environment while exploiting
its current knowledge.</p>
<h3 id="enhancing-the-agent-with-q-learning">Enhancing the Agent with
Q-Learning</h3>
<p>We upgrade our <code>RandomAgent</code> to a
<code>QLearningAgent</code> that can choose actions based on learned
Q-values and improve its decisions over time.</p>
<h4 id="file-q_learning_agent.py">File:
<code>q_learning_agent.py</code></h4>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># q_learning_agent.py</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QLearningAgent:</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, state_space, action_space, alpha, gamma, epsilon, action_mapping):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.q_table <span class="op">=</span> np.zeros((state_space, action_space))</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> alpha</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> gamma</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon <span class="op">=</span> epsilon</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action_space <span class="op">=</span> action_space</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action_mapping <span class="op">=</span> action_mapping  <span class="co"># Add this line to store the action mapping</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_action(<span class="va">self</span>, state):</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> random.uniform(<span class="dv">0</span>, <span class="dv">1</span>) <span class="op">&lt;</span> <span class="va">self</span>.epsilon:</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>            action_index <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="va">self</span>.action_space <span class="op">-</span> <span class="dv">1</span>)  <span class="co"># Explore action space</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>            action_index <span class="op">=</span> np.argmax(<span class="va">self</span>.q_table[state])  <span class="co"># Exploit learned values</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return the action index instead of the action itself</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> action_index</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_q_table(<span class="va">self</span>, current_state, action_index, reward, next_state):</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Find the best action at the next state</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        best_next_action <span class="op">=</span> np.argmax(<span class="va">self</span>.q_table[next_state])</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the target for the Bellman equation</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        td_target <span class="op">=</span> reward <span class="op">+</span> <span class="va">self</span>.gamma <span class="op">*</span> <span class="va">self</span>.q_table[next_state, best_next_action]</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the difference between the target and the current Q-value</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        td_delta <span class="op">=</span> td_target <span class="op">-</span> <span class="va">self</span>.q_table[current_state, action_index]</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update the Q-value for the current state-action pair</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.q_table[current_state, action_index] <span class="op">+=</span> <span class="va">self</span>.alpha <span class="op">*</span> td_delta</span></code></pre></div>
<h4 id="running-the-q-learning-agent">Running the Q-Learning Agent</h4>
<p>We will simulate the <code>QLearningAgent</code> within the
Gridworld, observing how it learns from experiences to improve its
navigation strategy.</p>
<h4 id="file-gridworld_q_learning.py">File:
<code>gridworld_q_learning.py</code></h4>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># gridworld_q_learning.py</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gridworld <span class="im">import</span> Gridworld</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> q_learning_agent <span class="im">import</span> QLearningAgent</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the Gridworld environment dimensions and attributes</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>grid_width <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>grid_height <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>num_states <span class="op">=</span> grid_width <span class="op">*</span> grid_height</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>num_actions <span class="op">=</span> <span class="dv">4</span>  <span class="co"># Up, down, left, and right</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Define action mapping that maps action indices to actual actions (movements)</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>action_mapping <span class="op">=</span> [(<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">0</span>), (<span class="dv">0</span>, <span class="dv">1</span>), (<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>)]  <span class="co"># Up, Right, Down, Left</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the Gridworld environment</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>gridworld <span class="op">=</span> Gridworld(width<span class="op">=</span>grid_width, height<span class="op">=</span>grid_height, start<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>), goal<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>), obstacles<span class="op">=</span>[(<span class="dv">1</span>, <span class="dv">1</span>), (<span class="dv">2</span>, <span class="dv">2</span>), (<span class="dv">3</span>, <span class="dv">3</span>)])</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the Q-Learning agent with hyperparameters alpha, gamma, and epsilon</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># Learning rate</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.9</span>  <span class="co"># Discount factor</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># Exploration rate</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> QLearningAgent(state_space<span class="op">=</span>num_states, action_space<span class="op">=</span>num_actions, alpha<span class="op">=</span>alpha, gamma<span class="op">=</span>gamma, epsilon<span class="op">=</span>epsilon, action_mapping<span class="op">=</span>action_mapping)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the Q-Learning agent in the Gridworld</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>num_episodes <span class="op">=</span> <span class="dv">20</span>  <span class="co"># Total number of episodes to run</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(num_episodes):</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    total_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    gridworld.reset()</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    current_state <span class="op">=</span> gridworld.get_state()</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Starting episode </span><span class="sc">{</span>episode <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Agent chooses an action index</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        action_index <span class="op">=</span> agent.choose_action(current_state)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Perform the chosen action and get the result</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>        _, reward, done <span class="op">=</span> gridworld.step(agent.action_mapping[action_index])</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert the next position to the next state index</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>        next_state <span class="op">=</span> gridworld.get_state()  <span class="co"># Assuming Gridworld has this method</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update the Q-table with the action index</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>        agent.update_q_table(current_state, action_index, reward, next_state)</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update total reward and current state</span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>        total_reward <span class="op">+=</span> reward</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>        current_state <span class="op">=</span> next_state</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Episode </span><span class="sc">{</span>episode <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">: Total Reward: </span><span class="sc">{</span>total_reward<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<h3 id="summary-3">Summary</h3>
<p>We’ve now set the theoretical foundation and implemented the
Q-Learning algorithm in our Gridworld agent. The agent’s ability to
learn from interactions with the environment will be demonstrated in the
following simulation.</p>
<h2 id="part-3-updating-the-gridworld-environment">Part 3: Updating the
Gridworld Environment</h2>
<h3 id="enhancing-state-representation">Enhancing State
Representation</h3>
<p>To facilitate our Q-Learning agent’s interaction with the Gridworld,
we must convert the environment’s grid coordinates into a state
index.</p>
<h3 id="objectives-2">Objectives</h3>
<ul>
<li>Enable the Gridworld to provide a state index representation for the
Q-Learning agent.</li>
<li>Update the Gridworld class with a <code>get_state()</code>
method.</li>
</ul>
<h3 id="the-get_state-method">The <code>get_state()</code> Method</h3>
<p>We add a method to the <code>Gridworld</code> class that will map the
agent’s 2D grid coordinates to a unique state index.</p>
<h3 id="implementation">Implementation</h3>
<p>We will implement the <code>get_state()</code> method to provide a
state representation by flattening the grid.</p>
<h4 id="file-gridworld.py">File: <code>gridworld.py</code></h4>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Gridworld:</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Existing methods...</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_state(<span class="va">self</span>):</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns a unique state index for the agent&#39;s current position.</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">=</span> <span class="va">self</span>.agent_position</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y <span class="op">*</span> <span class="va">self</span>.width <span class="op">+</span> x</span></code></pre></div>
<h3 id="summary-and-integration">Summary and Integration</h3>
<p>The <code>Gridworld</code> class now provides a state index that our
Q-Learning agent can use to update the Q-table and make decisions. This
integration is crucial for the upcoming simulations where the agent
learns to navigate the environment more effectively.</p>
<h3 id="next-steps">Next Steps</h3>
<p>With state representation in place, we can proceed to initialize the
Q-table and run the Q-Learning agent through training episodes to
observe its learning and performance improvements. The next sections of
this chapter will focus on these aspects, leading to a fully functional
Q-Learning agent in the Gridworld.</p>
<h1
id="chapter-3-fundamentals-of-deep-learning-for-reinforcement-learning">Chapter
3: Fundamentals of Deep Learning for Reinforcement Learning</h1>
<h2 id="overview-2">Overview</h2>
<p>This chapter delves into the synergy between deep learning and
reinforcement learning, leading to the burgeoning field of deep
reinforcement learning (DRL). We aim to elucidate the essential
principles of deep learning and show how neural networks, when paired
with RL, can effectively address intricate decision-making
challenges.</p>
<h2 id="objectives-3">Objectives</h2>
<ul>
<li>Understand the fundamental concepts of deep learning, with a focus
on neural networks.</li>
<li>Learn how deep learning can be integrated with RL algorithms to
improve their performance.</li>
<li>Prepare to implement a deep Q-network (DQN) in the Gridworld
project.</li>
</ul>
<h2 id="introduction-to-neural-networks">Introduction to Neural
Networks</h2>
<p>Neural networks are designed to mimic the workings of the human brain
and are made up of interconnected units called neurons. These networks
have the capacity to learn from data, enabling them to perform tasks
such as predictions or decision-making without the need for explicit
programming.</p>
<h3 id="key-concepts-in-neural-networks">Key Concepts in Neural
Networks</h3>
<ul>
<li><strong>Neurons</strong>: The building blocks of neural networks
that take input, process it using a weighted sum followed by an
activation function, and produce an output.</li>
<li><strong>Layers</strong>: A neural network comprises different layers
of neurons, which include the input layer, one or more hidden layers,
and the output layer.</li>
<li><strong>Weights and Biases</strong>: Parameters of the model that
adjust during training to make the neural network’s predictions as
accurate as possible.</li>
</ul>
<h2 id="how-neural-networks-learn">How Neural Networks Learn</h2>
<p>The process of learning in neural networks involves adjusting their
weights and biases. The goal is to reduce the discrepancy between the
model’s predictions and the actual target values.</p>
<ul>
<li><strong>Forward Propagation</strong>: This is the process where
input data is passed through the network to generate output
predictions.</li>
<li><strong>Loss Functions</strong>: These are measures used to assess
the deviation between the network’s predictions and the true
outcomes.</li>
<li><strong>Backpropagation</strong>: A key algorithm in neural network
training, backpropagation calculates the gradient of the loss function
with respect to each weight and bias, using these gradients to update
the parameters via gradient descent.</li>
</ul>
<h2 id="deep-learning-in-reinforcement-learning">Deep Learning in
Reinforcement Learning</h2>
<p>Deep learning equips RL with powerful function approximation
capabilities, essential for dealing with high-dimensional state or
action spaces.</p>
<h3 id="deep-reinforcement-learning">Deep Reinforcement Learning</h3>
<ul>
<li><strong>Function Approximation</strong>: The application of deep
neural networks to estimate value functions or policy functions.</li>
<li><strong>Advantages of Deep RL</strong>: These include the ability to
process complex inputs, generalize across different states, and discern
underlying patterns in the data.</li>
</ul>
<h2 id="enhancing-gridworld-with-deep-learning">Enhancing Gridworld with
Deep Learning</h2>
<p>To integrate deep learning into our Gridworld example, we will employ
a deep Q-network (DQN). A DQN utilizes a neural network to estimate the
Q-value function, a central concept in RL that predicts the quality of
actions taken in various states.</p>
<ul>
<li><strong>Project Update</strong>: We will implement a DQN to
substitute the Q-table with a neural network that predicts Q-values for
Gridworld.</li>
<li><strong>Implementation Steps</strong>: We outline the necessary
modifications to the existing Gridworld project to incorporate a
DQN.</li>
</ul>
<h2 id="summary-4">Summary</h2>
<p>We recap the deep learning concepts introduced in this chapter and
discuss their relevance to RL. This provides the foundation for a
hands-on implementation of a DQN in the following sections.</p>
<hr />
<h2 id="implementing-a-deep-q-network-dqn-with-pytorch">Implementing a
Deep Q-Network (DQN) with PyTorch</h2>
<p>We will utilize PyTorch, a widely-used deep learning framework, to
create our DQN model. PyTorch offers a comprehensive and intuitive
environment for constructing neural networks.</p>
<h3 id="file-gridworld_deep_q_learning.py">File:
<code>gridworld_deep_q_learning.py</code></h3>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gridworld <span class="im">import</span> Gridworld  <span class="co"># This imports the Gridworld environment</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the neural network architecture for Deep Q-Learning</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleMLP(nn.Module):</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Constructor for the neural network</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, output_size, hidden_size<span class="op">=</span><span class="dv">64</span>):</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SimpleMLP, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First fully connected layer from input_size to hidden_size</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, hidden_size)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Second fully connected layer, hidden to hidden</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(hidden_size, hidden_size)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Third fully connected layer from hidden_size to output_size</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(hidden_size, output_size)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass definition for the neural network</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply ReLU activation function after first layer</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply ReLU activation function after second layer</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output layer, no activation function</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Deep Q-Network agent class definition</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DQNAgent:</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Constructor for the DQN agent</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, output_size, hidden_size<span class="op">=</span><span class="dv">64</span>):</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The neural network model</span></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> SimpleMLP(input_size, output_size, hidden_size)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Epsilon for the epsilon-greedy policy (initially set to 1 for full exploration)</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Minimum value that epsilon can decay to over time</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon_min <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Rate at which epsilon is decayed over time</span></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon_decay <span class="op">=</span> <span class="fl">0.995</span></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># List to hold past experiences for replay</span></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.memory <span class="op">=</span> []</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Method to decide an action based on the current state</span></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> act(<span class="va">self</span>, state):</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check if we should take a random action (exploration)</span></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.random.rand() <span class="op">&lt;=</span> <span class="va">self</span>.epsilon:</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Return a random action within the action space size</span></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> random.randrange(output_size)</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If not exploring, process the state through the DQN to get the action values</span></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First, ensure state is a PyTorch tensor</span></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(state, torch.Tensor):</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> torch.from_numpy(state).<span class="bu">float</span>().unsqueeze(<span class="dv">0</span>)</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass through the network to get action values</span></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>        action_values <span class="op">=</span> <span class="va">self</span>.model(state)</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return the action with the highest value</span></span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.argmax(action_values.detach().numpy())</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Method to store experiences in replay memory</span></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> remember(<span class="va">self</span>, state, action, reward, next_state, done):</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Append the experience as a tuple to the memory list</span></span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.memory.append((state, action, reward, next_state, done))</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Method to decay epsilon over time for less exploration</span></span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_epsilon(<span class="va">self</span>):</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.epsilon <span class="op">&gt;</span> <span class="va">self</span>.epsilon_min:</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.epsilon <span class="op">*=</span> <span class="va">self</span>.epsilon_decay</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to train the neural network model</span></span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(agent, optimizer, batch_size, gamma):</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check that there are enough experiences in memory to sample a batch</span></span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(agent.memory) <span class="op">&lt;</span> batch_size:</span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample a minibatch of experiences from memory</span></span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>    minibatch <span class="op">=</span> random.sample(agent.memory, batch_size)</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Unpack the experiences</span></span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>    states, actions, rewards, next_states, dones <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>minibatch)</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert experience components to PyTorch tensors</span></span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a>    states <span class="op">=</span> torch.from_numpy(np.vstack(states)).<span class="bu">float</span>()</span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>    actions <span class="op">=</span> torch.from_numpy(np.vstack(actions)).<span class="bu">long</span>()</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>    rewards <span class="op">=</span> torch.from_numpy(np.vstack(rewards)).<span class="bu">float</span>()</span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a>    next_states <span class="op">=</span> torch.from_numpy(np.vstack(next_states)).<span class="bu">float</span>()</span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a>    dones <span class="op">=</span> torch.from_numpy(np.vstack(dones).astype(np.uint8)).<span class="bu">float</span>()</span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the expected Q values from the neural network</span></span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a>    Q_expected <span class="op">=</span> agent.model(states).gather(<span class="dv">1</span>, actions)</span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the Q value for the next states and get the max Q value for each next state</span></span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>    Q_targets_next <span class="op">=</span> agent.model(next_states).detach().<span class="bu">max</span>(<span class="dv">1</span>)[<span class="dv">0</span>].unsqueeze(<span class="dv">1</span>)</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the target Q values for the current states using the Bellman equation</span></span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a>    Q_targets <span class="op">=</span> rewards <span class="op">+</span> (gamma <span class="op">*</span> Q_targets_next <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> dones))</span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the loss between the expected Q values and the target Q values</span></span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.mse_loss(Q_expected, Q_targets)</span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backpropagate the loss, update the network weights</span></span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to log the performance of the agent</span></span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_performance(episode, total_reward, steps, epsilon):</span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print out the episode number, total reward, number of steps and the epsilon value</span></span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Episode: </span><span class="sc">{</span>episode<span class="sc">}</span><span class="ss">, Total Reward: </span><span class="sc">{</span>total_reward<span class="sc">}</span><span class="ss">, Steps: </span><span class="sc">{</span>steps<span class="sc">}</span><span class="ss">, Epsilon: </span><span class="sc">{</span>epsilon<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage of the defined classes and functions</span></span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the Gridworld environment</span></span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a>    gridworld <span class="op">=</span> Gridworld(width<span class="op">=</span><span class="dv">5</span>, height<span class="op">=</span><span class="dv">5</span>, start<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>), goal<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>), obstacles<span class="op">=</span>[(<span class="dv">1</span>, <span class="dv">1</span>), (<span class="dv">2</span>, <span class="dv">2</span>), (<span class="dv">3</span>, <span class="dv">3</span>)])</span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define action mapping that maps action indices to movements</span></span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a>    action_mapping <span class="op">=</span> [(<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">0</span>), (<span class="dv">0</span>, <span class="dv">1</span>), (<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>)]  <span class="co"># Up, Right, Down, Left</span></span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the input size based on the environment&#39;s state space</span></span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a>    input_size <span class="op">=</span> gridworld.width <span class="op">*</span> gridworld.height</span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The output size is the number of possible actions</span></span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a>    output_size <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the DQN agent</span></span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a>    agent <span class="op">=</span> DQNAgent(input_size, output_size)</span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define an optimizer for the neural network (Adam optimizer with a learning rate of 0.001)</span></span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(agent.model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Batch size for experience replay</span></span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Discount factor for future rewards</span></span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a>    gamma <span class="op">=</span> <span class="fl">0.99</span></span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Function to convert a state to a tensor for neural network input</span></span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> state_to_tensor(state, grid_size<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a one-hot encoded vector for the state</span></span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a>        state_vector <span class="op">=</span> torch.zeros(grid_size <span class="op">*</span> grid_size, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a>        state_vector[state] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> state_vector</span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Main training loop</span></span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reset the environment at the start of each episode</span></span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> gridworld.reset()</span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize total reward and steps counter</span></span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a>        total_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a>        done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a>        steps <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop for each step in the episode</span></span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Convert state to tensor format</span></span>
<span id="cb7-145"><a href="#cb7-145" aria-hidden="true" tabindex="-1"></a>            state_vector <span class="op">=</span> state_to_tensor(state)</span>
<span id="cb7-146"><a href="#cb7-146" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Select an action using the DQN agent&#39;s policy</span></span>
<span id="cb7-147"><a href="#cb7-147" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> agent.act(state_vector)</span>
<span id="cb7-148"><a href="#cb7-148" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Take the action in the environment and observe the next state and reward</span></span>
<span id="cb7-149"><a href="#cb7-149" aria-hidden="true" tabindex="-1"></a>            next_state, reward, done <span class="op">=</span> gridworld.step(action_mapping[action])</span>
<span id="cb7-150"><a href="#cb7-150" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Convert the next state to tensor format</span></span>
<span id="cb7-151"><a href="#cb7-151" aria-hidden="true" tabindex="-1"></a>            next_state_vector <span class="op">=</span> state_to_tensor(next_state)</span>
<span id="cb7-152"><a href="#cb7-152" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Remember the experience</span></span>
<span id="cb7-153"><a href="#cb7-153" aria-hidden="true" tabindex="-1"></a>            agent.remember(state_vector, action, reward, next_state_vector, done)</span>
<span id="cb7-154"><a href="#cb7-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-155"><a href="#cb7-155" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Move to the next state</span></span>
<span id="cb7-156"><a href="#cb7-156" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> next_state</span>
<span id="cb7-157"><a href="#cb7-157" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update the total reward</span></span>
<span id="cb7-158"><a href="#cb7-158" aria-hidden="true" tabindex="-1"></a>            total_reward <span class="op">+=</span> reward</span>
<span id="cb7-159"><a href="#cb7-159" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Increment the step counter</span></span>
<span id="cb7-160"><a href="#cb7-160" aria-hidden="true" tabindex="-1"></a>            steps <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb7-161"><a href="#cb7-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-162"><a href="#cb7-162" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Train the model with experiences from memory</span></span>
<span id="cb7-163"><a href="#cb7-163" aria-hidden="true" tabindex="-1"></a>            train_model(agent, optimizer, batch_size, gamma)</span>
<span id="cb7-164"><a href="#cb7-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-165"><a href="#cb7-165" aria-hidden="true" tabindex="-1"></a>        <span class="co"># After the episode, decay epsilon for less exploration in future episodes</span></span>
<span id="cb7-166"><a href="#cb7-166" aria-hidden="true" tabindex="-1"></a>        agent.update_epsilon()</span>
<span id="cb7-167"><a href="#cb7-167" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Log the performance metrics for the episode</span></span>
<span id="cb7-168"><a href="#cb7-168" aria-hidden="true" tabindex="-1"></a>        log_performance(episode, total_reward, steps, agent.epsilon)</span></code></pre></div>
<p>In this code snippet, we have outlined the creation of a simple
feedforward neural network model using PyTorch for approximating
Q-values in the Gridworld environment. The code includes a DQN agent
that selects actions using an epsilon-greedy policy and stores
experiences in a replay buffer. The <code>train_model</code> function
updates the neural network’s weights using sampled experiences to reduce
the loss between predicted and target Q-values.</p>
<p>Please note that this example is a simplified version of DQN. For
practical and more complex scenarios, additional mechanisms like
experience replay buffers with more sophisticated sampling techniques
and separate target networks to stabilize the Q-value predictions are
recommended.</p>
</body>
</html>
